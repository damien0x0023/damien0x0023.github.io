---
layout: post
title: Basic Econometrics - Unit 3 Multiple Regression Analysis - Inference
subtitle: åŸºç¡€è®¡é‡ç»æµå­¦ï¼ˆä¸‰ï¼‰
category: money
tags: [ECON5002]
---


translated by damien from Marco Avarucci

#  Basic Econometrics - Unit 3 Multiple Regression Analysis - Inference


- **Statistical inference in the regression model**  
&emsp;**å›å½’æ¨¡å‹ä¸­çš„ç»Ÿè®¡æ¨æ–­**

    - Hypothesis tests about population parameters.  
    &emsp;å…³äºæ€»ä½“å‚æ•°çš„å‡è®¾æ£€éªŒã€‚

    - Construction of confidence intervals.  
    &emsp;ç½®ä¿¡åŒºé—´çš„æ„é€ ã€‚

- **Sampling distributions of the OLS estimators**  
&emsp;**OLSä¼°è®¡é‡çš„æŠ½æ ·åˆ†å¸ƒ**

    - The OLS estimators are <u><font color=red>random variables</font></u>.  
    &emsp;OLSä¼°è®¡é‡æ˜¯éšæœºå˜é‡ã€‚

    - We already know their expected values and their variances.  
    &emsp;æˆ‘ä»¬å·²ç»çŸ¥é“äº†å®ƒä»¬çš„æœŸæœ›å€¼å’Œæ–¹å·®ã€‚

    - However, for hypothesis tests we need to know their <u><font color=red>distributions</font></u>.  
    &emsp;ç„¶è€Œï¼Œå¯¹äºå‡è®¾æ£€éªŒï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“å®ƒä»¬çš„åˆ†å¸ƒã€‚

    - In order to derive their distributions we need additional assumptions.  
    &emsp;ä¸ºäº†å¾—åˆ°å®ƒä»¬çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦é¢å¤–çš„å‡è®¾ã€‚

## Normal distribution in a nutshell  
&emsp;ç®€è¨€ä¹‹(?)ï¼Œæ­£æ€åˆ†å¸ƒ

![]({{site.url}}/assets/images/2020/ECON5002/normalDis.png "Normal Distribution 1")

![]({{site.url}}/assets/images/2020/ECON5002/normalDis2.png "Normal Distribution 2")

- If \\( X \sim N(\mu, \sigma ^2) \\), then  
&emsp;å¦‚æœä¸Šå¼æˆç«‹ï¼Œé‚£ä¹ˆ

$$
    Z = \frac{X - \mu}{\sigma} \sim N(0,1)
$$

- Let \\( X _1, \cdots, X _n\\) be mutually independent random variable with \\( X _i \sim N(\mu _i, \sigma _i^2) \\).  Let \\( a _1, \cdots, a _n \\) and \\( b _1, \cdots, b _n \\) be fixed constants. Then,  
&emsp;è®¾\\( X _1, \cdots, X _n\\) ä¸\\( X _i \sim N(\mu _i, \sigma _i^2) \\)æ˜¯ç›¸äº’ç‹¬ç«‹çš„éšæœºå˜é‡ã€‚è®©\\( a _1, \cdots, a _n \\)ä¸\\( b _1, \cdots, b _n \\)ä¸ºå›ºå®šå¸¸æ•°ã€‚é‚£ä¹ˆï¼Œ

$$
    Y = \sum _{i=1}^n (a _i X _i + b _i ) 
    \sim N \left( \sum _{i=1}^n (a _i \mu _i + b _i ), \sum _{i=1}^n a _i^2 \sigma _i^2 \right)
$$

- \\(ğ‘‹_1,ğ‘‹_2 \\) are jointly normal, then they are independent **IFF** \\(ğ¶ov(ğ‘‹ _1,ğ‘‹ _2) = 0 \\)  
&emsp;\\(ğ‘‹_1,ğ‘‹_2 \\) æ˜¯è”åˆæ­£æ€ï¼Œé‚£ä¹ˆä»–ä»¬æ˜¯ç‹¬ç«‹çš„**IFF**

- **Assumption MLR.6 (Normality of error terms)**  
&emsp;**å‡è®¾MLR.6ï¼ˆè¯¯å·®é¡¹çš„æ­£æ€æ€§ï¼‰**

$$
    u _i \sim N(0, \sigma ^2), \  u_i
    \text{ independent of } x _{i1}, x _{i2}, \cdots, x _{ik}
$$

![]({{site.url}}/assets/images/2020/ECON5002/mlr5.png "assumptionMLR6")

![]({{site.url}}/assets/images/2020/ECON5002/yMean.png "y Mean")

- **Discussion of the normality assumption**  
&emsp; **å…³äºæ­£æ€æ€§å‡è®¾çš„è®¨è®º**

    - The error term is the sum of *many* different unobserved factors.  
    &emsp;è¯¯å·®é¡¹æ˜¯*è®¸å¤š*ä¸åŒçš„æœªè§‚å¯Ÿå› ç´ çš„æ€»å’Œã€‚

    - Number large enough? Possibly very heterogenuous distributions of individual factors. How independent are the different factors?  
    &emsp;æ•°å­—å¤Ÿå¤§å—ï¼Ÿä¸ªä½“å› ç´ çš„åˆ†å¸ƒå¯èƒ½å¾ˆä¸å‡åŒ€ã€‚ä¸åŒçš„å› ç´ æœ‰å¤šç‹¬ç«‹ï¼Ÿ

    - The normality of the error term is an empirical question  
    &emsp;è¯¯å·®é¡¹çš„æ­£æ€æ€§æ˜¯ä¸€ä¸ªç»éªŒé—®é¢˜

    - At least the error distribution should be closeto normal  
    &emsp;è‡³å°‘è¯¯å·®åˆ†å¸ƒåº”è¯¥æ¥è¿‘æ­£æ€åˆ†å¸ƒ

    - In many cases, normality is questionable or impossible by definition Dependent variables are positive (wages), integer (# muders)....  
    &emsp;åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæ­£æ€æ€§æ˜¯æœ‰é—®é¢˜çš„æˆ–ä¸å¯èƒ½çš„å®šä¹‰å› å˜é‡æ˜¯æ­£çš„ï¼ˆå·¥èµ„ï¼‰ï¼Œæ•´æ•°ï¼ˆ#mudersï¼‰ã€‚ã€‚ã€‚ã€‚

    - Normality might be achieved trough transformation.  
    &emsp;æ­£æ€æ€§å¯ä»¥é€šè¿‡å˜æ¢æ¥å®ç°ã€‚

**Simulations**  
&emsp;**æ¨¡æ‹Ÿ**

![]({{site.url}}/assets/images/2020/ECON5002/simulations.png "Simulations")

- **Discussion of the normality assumption (cont.)**  
&emsp;**å…³äºæ­£æ€æ€§å‡è®¾çš„è®¨è®ºï¼ˆç»­ï¼‰**

    - Ultimately, normality is maintained for convenience.  
    &emsp;å½’æ ¹ç»“åº•ï¼Œä¸ºäº†æ–¹ä¾¿è€Œä¿æŒå¸¸æ€ã€‚

    - It allows to perform <u>exact</u> statistical inference.  
    &emsp;å®ƒå…è®¸æ‰§è¡Œç²¾ç¡®çš„ç»Ÿè®¡æ¨æ–­ã€‚

    - <u>Important:</u> The assumption of normality can be replaced by a large sample size.  
    &emsp;<u>é‡è¦æç¤º</u>:æ­£æ€æ€§å‡è®¾å¯ä»¥ç”¨å¤§æ ·æœ¬ä»£æ›¿ã€‚

### Example  
&emsp;ç¤ºä¾‹

- **the simple regression** \\( ğ’š=ğœ· _ğŸ + ğœ· _ğŸ ğ’™ + ğ’– \\)  
&emsp;**ç®€å•å›å½’**

$$
    \hat{\beta} _1 = \sum _{i=1}^n c _i y _i, 
    \to c _i = \frac{(x _i - \overline{x})}{\sum _{i=1}^n (x _i - \overline{x}) ^2}
$$

Note that  
&emsp;æ³¨æ„

$$
    \hat{\beta} _1 = \sum _{i=1}^n c _i (\beta _0 + \beta _1 x _i + u _i)
    = \sum _{i=1}^n c _i u _i
$$

because  
&emsp;å› ä¸º

$$
    \sum _{i=1}^n c _i = 0, \text{ and }
    \sum _{i=1^n} c _i x _i =1
$$

If \\( u _i \sim N (0, \sigma ^2) \\), then  
&emsp;å¦‚æœä¸Šå¼æˆç«‹ï¼Œé‚£ä¹ˆ

$$
    \hat{\beta} _1 | X \sim N \left (\beta _1, \frac{\sigma ^2}{\sum _{i=1}^n(x _i - \overline{x}) ^2} \right )
$$

- **Terminology**  
&emsp;**æœ¯è¯­**

$$
    \underbrace{MLR.1 - MLR.5} _{\text{"Gauss-Markov assumptions"}} \quad
    \underbrace{MLR.1 - MLR.6} _{\text{"Classical Linear Regression Model (CLRM) assumptions"}}
$$

- **Theorem 4.1 (Normal sampling distributions)**  
&emsp;**å®šç†4.1ï¼ˆæ­£æ€æŠ½æ ·åˆ†å¸ƒï¼‰**

Under assumptions MLR.1 â€“MLR.6:  
&emsp;åœ¨MLR.1-6çš„å‡è®¾ä¸‹

![]({{site.url}}/assets/images/2020/ECON5002/underAssumptions1to6.png "Under assumptions MLR.1 - MLR.6")

![]({{site.url}}/assets/images/2020/ECON5002/NDcruves.png "Normal Distribution cruves")

- <u> **Testing hypotheses about a single population parameter** </u>  
&emsp;<u> **æµ‹è¯•å…³äºå•ä¸ªæ€»ä½“å‚æ•°çš„å‡è®¾**</u>

- **Theorem 4.1 (t-distribution for standardized estimators)**  
&emsp;**å®šç†4.1ï¼ˆæ ‡å‡†åŒ–ä¼°è®¡çš„tåˆ†å¸ƒï¼‰**

Under assumptions MLR.1 â€“MLR.6:  
&emsp;åœ¨MLR.1-6çš„å‡è®¾ä¸‹

![]({{site.url}}/assets/images/2020/ECON5002/underAssumptions1to6_2.png "Theorem 4.1")

- **Null hypothesis (Standard in Econometric Softwares)**  
&emsp;**é›¶å‡è®¾ï¼ˆè®¡é‡ç»æµè½¯ä»¶ä¸­çš„æ ‡å‡†ï¼‰**

![]({{site.url}}/assets/images/2020/ECON5002/nullHypothesis.png "Null hypothesis")

## *T*-Distribution  
&emsp;*T*åˆ†å¸ƒ

![]({{site.url}}/assets/images/2020/ECON5002/NDc.png "t-Distribution cruves")

- **t-statistic (or t-ratio)**  
&emsp;**tç»Ÿè®¡ï¼ˆæˆ–è€…t-æ¯”ç‡ï¼‰**

![]({{site.url}}/assets/images/2020/ECON5002/tStatistic.png "T Statistic")

- **Distribution of the t-statistic <u>if the null hypothesis is true</u>**  
&emsp;**<u>å¦‚æœé›¶å‡è®¾ä¸ºçœŸï¼Œtç»Ÿè®¡é‡çš„åˆ†å¸ƒ</u>**

$$
    t _{\hat{\beta} _j} = \hat{\beta} _j / se( \hat{\beta} _j )
    = ( \hat{\beta} _j - \beta _j) / se( \hat{\beta} _j )
    \sim t _{n - k -1}
$$

- **Goal: Define a rejection rule so that, if it is true, H0is rejected only with a small probability (= significance level, e.g. 5%)** 
&emsp;**ç›®æ ‡ï¼šå®šä¹‰ä¸€ä¸ªæ‹’ç»è§„åˆ™ï¼Œè¿™æ ·ï¼Œå¦‚æœæ˜¯çœŸçš„ï¼Œh0è¢«æ‹’ç»çš„æ¦‚ç‡å¾ˆå°ï¼ˆ=æ˜¾è‘—æ€§æ°´å¹³ï¼Œä¾‹å¦‚5%ï¼‰**

### Examples  
&emsp;ç¤ºä¾‹

-   **Example: The sample average**  
&emsp;**ä¾‹å­ï¼šæ ·æœ¬å¹³å‡**

![]({{site.url}}/assets/images/2020/ECON5002/sampleAverage.png "The sample average")

- **Testing against one-sided alternatives (greater than zero)**  
&emsp;**é’ˆå¯¹å•å‘é€‰é¡¹ï¼ˆå¤§äºé›¶ï¼‰è¿›è¡Œæµ‹è¯•**

![]({{site.url}}/assets/images/2020/ECON5002/oneSidedAlternatives.png "The sample average")

- Specifying  
&emsp;æŒ‡å®š

$$
    H _1 : \beta _j > 0
$$

&emsp;&emsp;Means the null is effectively  
&emsp;è¡¨ç¤ºç©ºå€¼æ˜¯æœ‰æ•ˆçš„

$$
    H _0: \beta _j \le 0
$$

- If we reject \\( \beta _j = 0\\), then we reject any \\( \beta _j < 0\\)  
&emsp;å¦‚æœæˆ‘ä»¬æ‹’ç»ç›¸ç­‰å‡è®¾ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ‹’ç»ä»»ä½•è´Ÿå€¼å‡è®¾ã€‚

- We usually just state \\(H _0: \beta _j = 0 \\) and act like we don't care about negative values.  
&emsp;æˆ‘ä»¬é€šå¸¸åšå‡ºå¦‚ä¸Šå£°æ˜ï¼Œå°±åƒæˆ‘ä»¬ä¸å…³å¿ƒè´Ÿå€¼ã€‚

- If \\(\hat{\beta} _j \le 0\\), it provides no evidence against \\(H _0 \\)  
&emsp;å¦‚æœè¯¥å€¼å°äºç­‰äº0ï¼Œå®ƒæä¾›ä¸äº†è¯æ®å¯¹æŠ—é›¶å‡è®¾ã€‚

- If \\(\hat{\beta} _j > 0\\), how bid does \\(t _{\hat{\beta} _j}\\) have to be before we conclude that \\(H _0\\) is unlikely?  
&emsp;å¦‚æœè¯¥å€¼å¤§äº0ï¼Œåœ¨æˆ‘ä»¬å¾—å‡ºé›¶å‡è®¾ä¸å¤ªå¯èƒ½ä¹‹å‰ï¼Œ\\(t _{\hat{\beta} _j}\\)çš„å‡ºä»·åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿ

- <u>Traditional approach to hypothesis testing</u>:  
&emsp;<u> ä¼ ç»Ÿçš„å‡è®¾æ£€éªŒæ–¹æ³•</u>:

1. Choose a null hypothesis, e.g. \\( H _0 : \beta _j = 0 \\)  
&emsp;é€‰æ‹©ä¸€ä¸ªæ— æ•ˆå‡è®¾ï¼Œä¾‹å¦‚ä¸Šè¿°é›¶å‡è®¾ã€‚

2. Choose an alternative hypothesis, e.g. \\( H _0: \beta _j > 0 \\)  
&emsp;é€‰æ‹©å¦ä¸€ç§å‡è®¾ï¼Œä¾‹å¦‚ä¸ºæ­£å€¼ã€‚

3. Choose a significance level \\(\alpha \\)(level, size) for the test and compute the critical value \\( c _\alpha (Pr(t>c _\alpha) = \alpha)\\), so that the refection rule  
&emsp;ä¸ºæµ‹è¯•é€‰æ‹©æ˜¾è‘—æ€§æ°´å¹³\\(\alpha \\)ï¼ˆæ°´å¹³ï¼Œå¤§å°ï¼‰å¹¶è®¡ç®—ä¸´ç•Œå€¼\\( c _\alpha (Pr(t>c _\alpha) = \alpha)\\)ï¼Œä»¥ä¾¿åå°„è§„åˆ™

$$
    Reject \ \ if \ \  t _{\hat{\beta} _j}>c _\alpha
$$

&emsp;&emsp;leads to a \\( (\alpha \cdot 100) \\)% significance level.  
&emsp;&emsp;&emsp;å¯¼è‡´\\( (\alpha \cdot 100) \\)%%æ˜¾è‘—æ€§æ°´å¹³ã€‚ 

- The significance level \\(\alpha\\) is the probability of rejecting the null hypothesis when it is in fact true(Type I error)  
&emsp;æ˜¾è‘—æ€§æ°´å¹³\\(\alpha\\)æ˜¯åœ¨äº‹å®ä¸Šä¸ºçœŸæ—¶æ‹’ç»æ— æ•ˆå‡è®¾çš„æ¦‚ç‡ï¼ˆIå‹é”™è¯¯ï¼‰

- The probabilities of Type I and Type II errors cannot be minimized simultaneously.  
&emsp;ç±»å‹Iå’Œç±»å‹IIé”™è¯¯çš„æ¦‚ç‡ä¸èƒ½åŒæ—¶æœ€å°åŒ–ã€‚

- The classic approach is to keep \\(\alpha \\) at a fairly low level(10%, 5%, 1%)  
&emsp;ç»å…¸çš„æ–¹æ³•æ˜¯å°†\\(\alpha \\)ä¿æŒåœ¨ä¸€ä¸ªç›¸å½“ä½çš„æ°´å¹³ï¼ˆ10%ï¼Œ5%ï¼Œ1%ï¼‰

![]({{site.url}}/assets/images/2020/ECON5002/statisticiansCorner.png "Type I and II errors")

![]({{site.url}}/assets/images/2020/ECON5002/criticalValues.png "Critical Values of the t-Distribution")

- **Example: Wage equation**  
&emsp;**ç¤ºä¾‹ï¼šå·¥èµ„å…¬å¼**

    -   Test whether, after controlling for education and tenure, higher work experience leads to higher hourly wages  
    &emsp;æµ‹è¯•åœ¨æ§åˆ¶äº†æ•™è‚²å’Œä»»æœŸåï¼Œæ›´é«˜çš„å·¥ä½œç»éªŒæ˜¯å¦ä¼šå¯¼è‡´æ›´é«˜çš„å°æ—¶å·¥èµ„

![]({{site.url}}/assets/images/2020/ECON5002/wageEqu.png "Wage equation")

![]({{site.url}}/assets/images/2020/ECON5002/wageEqu2.png "Wage equation(cont.)")

<u>"The effect of experience on hourly wage is statistically greater than zero at the 5% (and even at the 1%) significance level."</u>  
&emsp;<u> â€œåœ¨5%ï¼ˆç”šè‡³1%ï¼‰æ˜¾è‘—æ€§æ°´å¹³ä¸Šï¼Œç»éªŒå¯¹å°æ—¶å·¥èµ„çš„å½±å“åœ¨ç»Ÿè®¡å­¦ä¸Šå¤§äºé›¶ã€‚â€</u>
 
- **Testing against one-sided alternatives (**less **than zero)**  
**é’ˆå¯¹å•ä¾§å¤‡é€‰æ–¹æ¡ˆè¿›è¡Œæµ‹è¯•ï¼ˆ**å°äº**å°äºé›¶ï¼‰**

![]({{site.url}}/assets/images/2020/ECON5002/testOneSided.png "Testing against one-sided alternatives")

- **Example: Student performance and school size**  
&emsp;**ä¾‹å¦‚ï¼šå­¦ç”Ÿè¡¨ç°å’Œå­¦æ ¡è§„æ¨¡**

    -   Test whether smaller school size leads to better student performance  
    &emsp;æµ‹è¯•è¾ƒå°çš„å­¦æ ¡è§„æ¨¡æ˜¯å¦èƒ½æé«˜å­¦ç”Ÿçš„å­¦ä¹ æˆç»©

![]({{site.url}}/assets/images/2020/ECON5002/schoolSizeStudentPerformance.png "smaller school size leads to better student performance?")

![]({{site.url}}/assets/images/2020/ECON5002/schoolSizeStudentPerformance2.png "Student performance and school size (cont.)")

<u>One cannot reject the hypothesis that there is no effect of school size on student performance (not even for a large significance level of 15%). \\( \hat{\beta} _{enroll} \\) is <font color=red>statistically insignificant</font> at 15% significance level.</u>  
&emsp;<u>æˆ‘ä»¬ä¸èƒ½å¦è®¤å­¦æ ¡è§„æ¨¡å¯¹å­¦ç”Ÿæˆç»©æ²¡æœ‰å½±å“çš„å‡è®¾ï¼ˆå³ä½¿æ˜¯15%çš„æ˜¾è‘—æ€§æ°´å¹³ä¹Ÿæ²¡æœ‰å½±å“ï¼‰ã€‚\\( \hat{\beta} _{enroll} \\)åœ¨15%æ˜¾è‘—æ€§æ°´å¹³ä¸Šæ— ç»Ÿè®¡å­¦æ„ä¹‰ã€‚</u>

-   Alternative specification of functional form:  
&emsp;å‡½æ•°å½¢å¼çš„æ›¿ä»£è§„èŒƒï¼š

![]({{site.url}}/assets/images/2020/ECON5002/alternativeForm.png "Alternative specification of functional form")

![]({{site.url}}/assets/images/2020/ECON5002/tStaCritVal.png "t-statistic and Critical value for 5% significance level")

<u>The hypothesis that there is no effect of school size on student performance can be rejected in favor of the hypothesis that the effect is negative.</u>  
&emsp;<u> å­¦æ ¡è§„æ¨¡å¯¹å­¦ç”Ÿæˆç»©æ²¡æœ‰å½±å“çš„å‡è®¾å¯èƒ½ä¼šè¢«å¦å®šï¼Œè€Œæ”¯æŒæ¶ˆæå½±å“çš„å‡è®¾ã€‚</u>

![]({{site.url}}/assets/images/2020/ECON5002/effect.png "How large is the effect?")

- **Testing against two-sided alternatives**  
&emsp;**æµ‹è¯•åŒé¢æ›¿ä»£å“**

![]({{site.url}}/assets/images/2020/ECON5002/testTwoSided.png "Testing against Two-sided alternatives")

- Example: Determinants of college GPA  
&emsp;ä¾‹å¦‚ï¼šå¤§å­¦å¹³å‡ç»©ç‚¹çš„å†³å®šå› ç´ 

![]({{site.url}}/assets/images/2020/ECON5002/GPADeterminants.png "Determinants of colleage GPA")

- **"Statistically significantâ€œ variables in a regression**  
&emsp;**å›å½’ä¸­çš„â€œç»Ÿè®¡æ˜¾è‘—â€å˜é‡**

    -   If a regression coefficient is different from zero in a two-sided test, the corresponding variable is said to be "statistically significant"  
    &emsp;å¦‚æœåœ¨åŒè¾¹æ£€éªŒä¸­å›å½’ç³»æ•°ä¸åŒäºé›¶ï¼Œåˆ™ç›¸åº”å˜é‡ç§°ä¸ºâ€œç»Ÿè®¡æ˜¾è‘—æ€§â€

    -   If the number of degrees of freedom is large enough so that the normal approximation applies, the following rules of thumb apply:  
    &emsp;å¦‚æœè‡ªç”±åº¦çš„æ•°ç›®è¶³å¤Ÿå¤§ï¼Œä»¥è‡´äºæ³•å‘è¿‘ä¼¼é€‚ç”¨ï¼Œåˆ™ä»¥ä¸‹ç»éªŒæ³•åˆ™é€‚ç”¨ï¼š

    $$
        \begin{array}{l}
        |t-ratio| > 1.645 & \to \text{ "statistically significant at 10% level"}
        \\\\ |t-ratio| > 1.96 & \to \text{ "statistically significant at 5% level"}
        \\\\ |t-ratio| > 2.576 & \to \text{ "statistically significant at 1% level"}
        \end{array}
    $$

<u>Remarks:</u>  
&emsp;å¤‡æ³¨ï¼š

- Our hypothesis involve the unknown \\(\beta _j, \ j = 1, \cdots, k \\)  
&emsp;æˆ‘ä»¬çš„å‡è®¾æ¶‰åŠæœªçŸ¥çš„\\(\beta _j, \ j = 1, \cdots, k \\)

- Assume that \\(\hat{\beta} _j = 2.75 \\). We don't write the null hypothesis as  
&emsp;å‡è®¾\\(\hat{\beta} _j = 2.75 \\)ã€‚æˆ‘ä»¬ä¸æŠŠé›¶å‡è®¾å†™æˆ

$$
    H _0 : 2.75 =0
$$

- Nor do we write  
&emsp;æˆ‘ä»¬ä¹Ÿä¸å†™

$$
    H _0 : \hat{\beta} _j = 0
$$

We are testing if 2.75 is likely to be a realization of a normally distribution random variable centred in \\(\beta _j \\) with variance \\(Var(\hat{\beta} _j)\\).  
&emsp;æˆ‘ä»¬æ­£åœ¨æµ‹è¯•2.75æ˜¯å¦å¯èƒ½æ˜¯ä¸€ä¸ªæ­£æ€åˆ†å¸ƒéšæœºå˜é‡çš„å®ç°ï¼Œå…¶ä¸­å¿ƒä¸º\\(\beta _j \\)ï¼Œæ–¹å·®ä¸º\\(Var(\hat{\beta} _j)\\)ã€‚


- **Practical versus Statistical Significance**  
&emsp;**å®é™…æ„ä¹‰ä¸ç»Ÿè®¡æ„ä¹‰**

    -   t-testing is purely about <u>statistical significance</u>  
    &emsp;tæ£€éªŒçº¯ç²¹æ˜¯å…³äºç»Ÿè®¡æ˜¾è‘—æ€§

    -   <u>*Practical(Economic) significance*</u> depends on the size and sign of \\(\hat{\beta} _j \\)  
    &emsp;å®é™…ï¼ˆç»æµï¼‰æ„ä¹‰å–å†³äº\\(\hat{\beta} _j \\)çš„å¤§å°å’Œç¬¦å·

    -   It is possible to estimate large(meaningful) economic effects but have the estimates so imprecise that they are statistically insignificant(small dataset, multicollinearity).  
    &emsp;æœ‰å¯èƒ½ä¼°è®¡å‡ºå¤§çš„ï¼ˆæœ‰æ„ä¹‰çš„ï¼‰ç»æµå½±å“ï¼Œä½†ç”±äºä¼°è®¡å¤ªä¸ç²¾ç¡®ï¼Œæ‰€ä»¥åœ¨ç»Ÿè®¡ä¸Šä¸é‡è¦ï¼ˆå°æ•°æ®é›†ï¼Œå¤šé‡å…±çº¿æ€§ï¼‰ã€‚

    -   It is possible to get estimates that are statistically significant(small p-values) but are not practically large.  
    &emsp;å¯ä»¥å¾—åˆ°å…·æœ‰ç»Ÿè®¡æ„ä¹‰ï¼ˆå°på€¼ï¼‰ä½†å®é™…ä¸Šå¹¶ä¸å¤§çš„ä¼°è®¡å€¼ã€‚

    - Do not just fixate on *t statistics*! Interpreting the \\(\hat{\beta} _j \\) is just as important.  
    &emsp;ä¸è¦åªä¸“æ³¨äº*tç»Ÿè®¡æ•°æ®*ï¼è§£é‡Š\\(\hat{\beta} _j \\)åŒæ ·é‡è¦ã€‚

-   <u>Testing more general hypotheses about a regression coefficient</u>  
&emsp;æ£€éªŒå…³äºå›å½’ç³»æ•°çš„æ›´ä¸€èˆ¬çš„å‡è®¾

![]({{site.url}}/assets/images/2020/ECON5002/HypoCoefficient.png "Null hypothesis and t-statistic")

-   <u>**The test works exactly as before, except that the hypothesized value is substracted from the estimate when forming the statistic**</u>  
&emsp;<u>**é™¤äº†åœ¨å½¢æˆç»Ÿè®¡æ•°æ®æ—¶ä»ä¼°è®¡å€¼ä¸­å‡å»å‡è®¾å€¼å¤–ï¼Œæµ‹è¯•çš„å·¥ä½œåŸç†ä¸ä¹‹å‰å®Œå…¨ç›¸åŒ**</u>

- **Example: Campus crime and enrollment**  
&emsp;**ä¾‹å¦‚ï¼šæ ¡å›­çŠ¯ç½ªå’Œæ‹›ç”Ÿ**

    -   An interesting hypothesis is whether crime increases by one percent if enrollment is increased by one percent  
    &emsp;ä¸€ä¸ªæœ‰è¶£çš„å‡è®¾æ˜¯ï¼Œå¦‚æœå…¥å­¦äººæ•°å¢åŠ 1%ï¼ŒçŠ¯ç½ªç‡æ˜¯å¦ä¼šå¢åŠ 1%

![]({{site.url}}/assets/images/2020/ECON5002/crimeEnrollment.png "Campus crime and enrollment")

&emsp;Language:\\(\hat{\beta} _{log(enroll)}\\) is statistically different from 1 at 5% level.  
&emsp;&emsp;è¯­è¨€ï¼š\\(\hat{\beta} _{log(enroll)}\\) ä¸1åœ¨5%æ°´å¹³ä¸Šæœ‰ç»Ÿè®¡å­¦å·®å¼‚ã€‚

- <u>**Computing p-values for t-tests**</u>  
&emsp;<u>**è®¡ç®—tæ£€éªŒçš„på€¼**</u>

    -   If the significance level is made smaller and smaller, there will be a point where the null hypothesis cannot be rejected anymore  
    &emsp;å¦‚æœæ˜¾è‘—æ€§æ°´å¹³è¶Šæ¥è¶Šå°ï¼Œå°±ä¼šå‡ºç°ä¸€ä¸ªä¸èƒ½å†æ‹’ç»æ— æ•ˆå‡è®¾çš„ç‚¹

    -   Lowering the significance level reduces Pr(Type I err)  
    &emsp;é™ä½æ˜¾è‘—æ€§æ°´å¹³é™ä½Prï¼ˆIå‹é”™è¯¯ï¼‰

    -   <font color=red><u>Definition</u></font>:The smallest significance level at which the null hypothesis is still rejected, is called the p-valueof the hypothesis test  
    &emsp;<font color=red><u>å®šä¹‰</u></font>ï¼šæ— æ•ˆå‡è®¾ä»è¢«æ‹’ç»çš„æœ€å°æ˜¾è‘—æ€§æ°´å¹³ï¼Œç§°ä¸ºå‡è®¾æ£€éªŒçš„på€¼

    -   A small p-value is evidence against the null hypothesis because one would reject the null hypothesis even at small significance levels  
    &emsp;ä¸€ä¸ªå°çš„på€¼æ˜¯åå¯¹æ— æ•ˆå‡è®¾çš„è¯æ®ï¼Œå› ä¸ºå³ä½¿åœ¨å¾ˆå°çš„æ˜¾è‘—æ€§æ°´å¹³ä¸Šï¼Œäººä»¬ä¹Ÿä¼šæ‹’ç»æ— æ•ˆå‡è®¾

    -   A large p-value is evidence in favor of the null hypothesis  
    &emsp;ä¸€ä¸ªå¤§çš„på€¼æ˜¯æ”¯æŒæ— æ•ˆå‡è®¾çš„è¯æ®

    -   P-values are more informative than tests at fixed significance levels  
    &emsp;På€¼æ¯”å›ºå®šæ˜¾è‘—æ€§æ°´å¹³ä¸‹çš„æ£€éªŒæ›´å…·ä¿¡æ¯æ€§

- **How the p-value is computed (here: two-sided test)**  
&emsp;**å¦‚ä½•è®¡ç®—på€¼ï¼ˆæ­¤å¤„ï¼šåŒè¾¹è¯•éªŒï¼‰**

![]({{site.url}}/assets/images/2020/ECON5002/twoSidedTest.png "How the p-value is computed in two-sided test")

- Given a *p-value*, we can carry out a test at any significance level  
&emsp;ç»™å®šä¸€ä¸ªpå€¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•æ˜¾è‘—æ€§æ°´å¹³ä¸Šè¿›è¡Œæ£€éªŒ

- If \\(\alpha \\) is the chosen level, then  
&emsp;å¦‚æœ\\ï¼ˆ\alpha\\ï¼‰æ˜¯æ‰€é€‰çº§åˆ«ï¼Œåˆ™

$$
    \text{Reject} H _0 \text{  if  } p-value < \alpha \text{,    Do not reject } H _0 \text{  if  } p-value > \alpha
$$

- Example: Student performance and sccol size(one sided, n=408)  
&emsp;ç¤ºä¾‹ï¼šå­¦ç”Ÿè¡¨ç°å’Œsccolå¤§å°ï¼ˆå•ä¾§ï¼Œn=408ï¼‰

$$
    \begin{array}{m}
        t _{log(enroll)} = -1.29/.69 \approx -1.87
        \\\\ Pr(t \le -1.87) = Pr(t \ge 1.87) = 0.0307
    \end{array}
$$

&emsp;The null is rejected at 5%, it is not rejected at 1%  
&emsp;ç©ºå‡è®¾åœ¨5%æ—¶è¢«æ‹’ç»ï¼Œåœ¨1%æ—¶ä¸è¢«æ‹’ç»

- Eviews reports two sided *p-value*. Divide by two to obtain one-sided *p-value*.  
&emsp;EviewsæŠ¥å‘ŠåŒé¢*på€¼*ã€‚é™¤ä»¥äºŒå¾—åˆ°å•é¢çš„*på€¼*ã€‚

![]({{site.url}}/assets/images/2020/ECON5002/criticalValues.png "Critical Values of the t-Distribution 2")

<u>**Confidence Intervals**</u>  
&emsp;<u> **ç½®ä¿¡åŒºé—´**</u>

-   The CI (also know as interval estimate) is supposed to give a â€œlikelyâ€ range of values for the population parameters.  
&emsp;CIï¼ˆä¹Ÿç§°ä¸ºåŒºé—´ä¼°è®¡ï¼‰åº”è¯¥ç»™å‡ºæ€»ä½“å‚æ•°çš„â€œå¯èƒ½â€å€¼èŒƒå›´ã€‚

-   The \\(\alpha % \\) confidence interval for \\(\beta _j \\) , is of the form  
&emsp;\\ï¼ˆ\beta\u j\\ï¼‰çš„\\ï¼ˆ\alpha%\\ï¼‰ç½®ä¿¡åŒºé—´çš„å½¢å¼å¦‚ä¸‹

$$
    \hat{\beta} _j \pm c _\alpha \cdot se(\hat{\beta} _j)
$$

-   For 95% CI, \\(c _\alpha \\) comes from the 97.5 percentile in the \\( t _{df}\\) distribution.  
&emsp;å¯¹äº95%ç½®ä¿¡åŒºé—´ï¼Œ\\ï¼ˆc \\ alpha\\ï¼‰æ¥è‡ªäº\\ï¼ˆt{df}\\ï¼‰åˆ†å¸ƒçš„97.5ä¸ªç™¾åˆ†ä½æ•°ã€‚

-   The width of the CI depends on precision of the estimates and confidence level.  
&emsp;ç½®ä¿¡åŒºé—´çš„å®½åº¦å–å†³äºä¼°è®¡çš„ç²¾åº¦å’Œç½®ä¿¡æ°´å¹³ã€‚

## F-Distribution  
&emsp;##Fåˆ†å¸ƒ

-   Let \\(\alpha = 5%\\),  
&emsp;è®¾\\(\alpha = 5%\\)ï¼Œ

![]({{site.url}}/assets/images/2020/ECON5002/CIcalc.png "Confidence Level Calculation")

-   **Interpretation of the confidence interval**  
&emsp;**ç½®ä¿¡åŒºé—´çš„è§£é‡Š**

    -   The bounds of the interval are random  
    &emsp;åŒºé—´çš„ç•Œé™æ˜¯éšæœºçš„

    -   In repeated samples, the interval that is constructed in the above way will cover the population regression coefficient in 95% of the casess  
    &emsp;åœ¨é‡å¤æ ·æœ¬ä¸­ï¼Œç”¨ä¸Šè¿°æ–¹æ³•æ„é€ çš„åŒºé—´å°†è¦†ç›–95%çš„æ ·æœ¬çš„æ€»ä½“å›å½’ç³»æ•°

-   Confidence intervals for typical confidence levels  
&emsp;å…¸å‹ç½®ä¿¡æ°´å¹³çš„ç½®ä¿¡åŒºé—´

![]({{site.url}}/assets/images/2020/ECON5002/typicalCI.png "CI for typical confidence levels")

-   Relationship between confidence intervals and hypotheses tests  
&emsp;ç½®ä¿¡åŒºé—´ä¸å‡è®¾æ£€éªŒçš„å…³ç³»

$$
    \alpha \notin interval \Rightarrow \text{reject} H _0: \beta _j = a _j \text{ in favor of } H _1 : \beta _j \neq a _j
$$

![]({{site.url}}/assets/images/2020/ECON5002/betaVal.png "values of beta")

- **Example: Model of firmsâ€˜ R&D expenditures**  
&emsp;*ç¤ºä¾‹ï¼šä¼ä¸šç ”å‘æ”¯å‡ºæ¨¡å‹**

![]({{site.url}}/assets/images/2020/ECON5002/RandDExpenditures.png "Firm's R&D expenditures")

-   One often sees statements such as â€œthere is a 95%â€ chance that \\(\beta _{log(sales)} \\) is in the interval [0.961,1.21]  
&emsp;äººä»¬ç»å¸¸ä¼šçœ‹åˆ°è¿™æ ·çš„è¯­å¥ï¼šâ€œæœ‰95%çš„å¯èƒ½æ€§ï¼Œ\\(\beta _{log(sales)} \\)åœ¨åŒºé—´[0.961,1.21]

-   This is incorrect. \\(\beta _{log(sales)} \\)  is a fixed (unknown)value, and it either is or is not in the interval.  
&emsp;è¿™æ˜¯ä¸æ­£ç¡®çš„ã€‚\\(\beta _{log(sales)} \\)æ˜¯ä¸€ä¸ªå›ºå®šçš„ï¼ˆæœªçŸ¥ï¼‰å€¼ï¼Œå®ƒè¦ä¹ˆåœ¨åŒºé—´å†…ï¼Œè¦ä¹ˆä¸åœ¨åŒºé—´å†…ã€‚

-   For a particular sample, we donâ€™t know whether \\(\beta _{log(sales)} \\)  is in the interval  
&emsp;å¯¹äºä¸€ä¸ªç‰¹å®šçš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬ä¸çŸ¥é“\\(\beta _{log(sales)} \\)æ˜¯å¦åœ¨é—´éš”å†…

-   <u>**Testing Single Linear Restrictions**</u>  
&emsp;<u>**æµ‹è¯•å•ä¸€çº¿æ€§é™åˆ¶**</u>

### Example  
&emsp;ç¤ºä¾‹

-   **Return to education at 2 year vs. at 4 year colleges**  
&emsp;**2å¹´åˆ¶å¤§å­¦ä¸4å¹´åˆ¶å¤§å­¦çš„å›å½’æ•™è‚²**

![]({{site.url}}/assets/images/2020/ECON5002/2or4yearsEdu.png "education at 2 years vs. at 4 years colleages")

-   **Impossible to compute with standard regression output because**  
&emsp;**æ— æ³•ä½¿ç”¨æ ‡å‡†å›å½’è¾“å‡ºè¿›è¡Œè®¡ç®—ï¼Œå› ä¸º**

![]({{site.url}}/assets/images/2020/ECON5002/imposCompu.png "not available in regression output")

-   **Alternative method**  
&emsp;**æ›¿ä»£æ–¹æ³•**

![]({{site.url}}/assets/images/2020/ECON5002/alterMethod.png "Alternative method")

-   **Estimation results**  
&emsp;**ä¼°ç®—ç»“æœ**

![]({{site.url}}/assets/images/2020/ECON5002/estRes.png "Estimation Results")

-   **This method works <u>always</u> for single linear hypotheses**  
&emsp;**è¿™ç§æ–¹æ³•å¯¹å•ä¸ªçº¿æ€§å‡è®¾æ€»æ˜¯æœ‰æ•ˆçš„**

-   <u>**Testing multiple linear restrictions: The F-test**</u>  
&emsp;<u> **æµ‹è¯•å¤šé‡çº¿æ€§é™åˆ¶ï¼šFæµ‹è¯•**</u>  

-   **Testing exclusion restrictions**  
&emsp;**æµ‹è¯•æ’é™¤é™åˆ¶**

![]({{site.url}}/assets/images/2020/ECON5002/exclusionRestrictions.png "Testing exclusion restrictions")

-   **Estimation of the unrestricted model**  
&emsp;**éé™åˆ¶æ¨¡å‹çš„é¢„æµ‹**

![]({{site.url}}/assets/images/2020/ECON5002/unrestrictedEst.png "Estimation of the unrestricted model")

![]({{site.url}}/assets/images/2020/ECON5002/unrestrictedEst2.png "Test Statistic")

-   **Test statistic**
&emsp;**æµ‹è¯•ç»Ÿè®¡**

![]({{site.url}}/assets/images/2020/ECON5002/testStatistic.png "Estimation of the unrestricted model")

-   **Rejection rule(Figure)**  
&emsp;**æ‹’ç»è§„åˆ™ï¼ˆå›¾ï¼‰**

![]({{site.url}}/assets/images/2020/ECON5002/rejectionRule.png "Figure 4.7")

![]({{site.url}}/assets/images/2020/ECON5002/criticalValuesFDis.png "5% Critical values of the F Distribution")

![]({{site.url}}/assets/images/2020/ECON5002/FDis.png "F Distribution")

-   **Test decision in the example**  
**ç¤ºä¾‹ä¸­çš„æµ‹è¯•å†³ç­–**

![]({{site.url}}/assets/images/2020/ECON5002/testDesicion.png "Test decision in the example")

-   **Discussion**  
&emsp;**è®¨è®º**

    -   The three variables are "jointly significant"  
    &emsp;è¿™ä¸‰ä¸ªå˜é‡â€œå…±åŒæ˜¾è‘—â€ã€‚

    -   They were not significant when tested individually. Should we conclude that none of *bavg*, *hyrunsyr*, and *rbisyr* affects baseball player salaries? NO. This could be a mistake.  
    &emsp;å•ç‹¬æµ‹è¯•æ—¶ï¼Œå®ƒä»¬å¹¶ä¸æ˜¾è‘—ã€‚æˆ‘ä»¬æ˜¯å¦åº”è¯¥å¾—å‡ºè¿™æ ·çš„ç»“è®ºï¼šbavg*ã€*hyrunsyr*å’Œ*rbisyr*éƒ½ä¸ä¼šå½±å“æ£’çƒè¿åŠ¨å‘˜çš„å·¥èµ„ï¼Ÿä¸ï¼Œè¿™å¯èƒ½æ˜¯ä¸ªé”™è¯¯ã€‚

    -   The likely reason is multicollinearity between them (STANDARD ERRORS!)  
    &emsp;å¯èƒ½çš„åŸå› æ˜¯å®ƒä»¬ä¹‹é—´çš„å¤šé‡å…±çº¿æ€§ï¼ˆæ ‡å‡†è¯¯å·®ï¼ï¼‰

    -   Corr(*hyrunsyr*,*rbisyr* )=0.89. One cannot hit a home run without getting at least one run batted in.  
    &emsp;Corrï¼ˆ*hyrunsyr*ï¼Œ*rbisyr*ï¼‰=0.89ã€‚ä¸€ä¸ªäººä¸å¯èƒ½æ‰“å‡ºä¸€ä¸ªæœ¬å’æ‰“è€Œä¸å¾—åˆ°è‡³å°‘ä¸€æ¬¡å‡»çƒã€‚


##  Joint Hypotheses Test  
&emsp;è”åˆå‡è®¾æ£€éªŒ

-   We want to see how the fit deteriorates as we remove a subset of variables **(joint hypotheses test)**.  
&emsp;æˆ‘ä»¬æƒ³çœ‹çœ‹å½“æˆ‘ä»¬å»æ‰ä¸€éƒ¨åˆ†å˜é‡**ï¼ˆè”åˆå‡è®¾æ£€éªŒï¼‰**æ—¶ï¼Œæ‹Ÿåˆåº¦æ˜¯å¦‚ä½•æ¶åŒ–çš„ã€‚

-   \\(SSR _r \ge SSR _{ur} \\)(algebraic fact).  
\\(SSR _r \ge SSR _{ur} \\)ï¼ˆä»£æ•°äº‹å®ï¼‰ã€‚

-   The F-test essentially ask: does the SSR increase proportionally by enough to conclude the restrictions under \\(H _0\\) are false?  
&emsp;Fæ£€éªŒæœ¬è´¨ä¸Šæ˜¯åœ¨é—®ï¼šSSRæ˜¯å¦æŒ‰æ¯”ä¾‹å¢åŠ åˆ°è¶³ä»¥å¾—å‡ºç»“è®ºï¼Œåœ¨\\(H _0\\)ä¸‹çš„é™åˆ¶æ˜¯é”™è¯¯çš„ï¼Ÿ

-   The F-statistic use a degree of freedom adjustment.  
&emsp;Fç»Ÿè®¡é‡ä½¿ç”¨è‡ªç”±åº¦è°ƒæ•´ã€‚

-   If the null is rejected, the test will not tell us which parameter is different from zero.  
&emsp;å¦‚æœnullè¢«æ‹’ç»ï¼Œæµ‹è¯•å°†ä¸ä¼šå‘Šè¯‰æˆ‘ä»¬å“ªä¸ªå‚æ•°ä¸åŒäºé›¶ã€‚

-   <u>**Test of overall significance of a regression**</u>  
&emsp;<u>**å›å½’æ€»ä½“æ˜¾è‘—æ€§æ£€éªŒ**</u>

![]({{site.url}}/assets/images/2020/ECON5002/overallSignificance.png "Test of overall significance of a regression")

-   **The test of overall significance is reported in most regression packages; the null hypothesis is usually overwhelmingly rejected**  
&emsp;*åœ¨å¤§å¤šæ•°å›å½’åŒ…ä¸­æŠ¥å‘Šäº†æ€»ä½“æ˜¾è‘—æ€§æ£€éªŒï¼›æ— æ•ˆå‡è®¾é€šå¸¸è¢«å‹å€’æ€§åœ°æ‹’ç»**

-   <u>**Testing general linear restrictions with the F-test**</u>  
&emsp;<u>**ç”¨F-æ£€éªŒæ³•æ£€éªŒä¸€èˆ¬çº¿æ€§é™åˆ¶æ¡ä»¶**</u>

### Example  
&emsp;ç¤ºä¾‹

-   **Test whether house price assessments are rational**  
&emsp;**æ£€éªŒæˆ¿ä»·è¯„ä¼°æ˜¯å¦åˆç†**

![]({{site.url}}/assets/images/2020/ECON5002/rationalHousePrice.png "Test whether house price assessments are rational")

-   Regression output for the unrestricted regression  
&emsp;æ— é™åˆ¶å›å½’çš„å›å½’è¾“å‡º

![]({{site.url}}/assets/images/2020/ECON5002/unrestrictedRegOut.png "Regression output for the unrestricted regression")

-   Square sum restricted resduals  
&emsp;å¹³æ–¹å’Œé™åˆ¶é‡æ•°

$$
    \begin{array}{l}
    x _i = log(price _i) - log(assess _i) \text{,  }
    SSR _r = \sum _{i=1}^n (x _i - \overline{x}) ^2 = 1.880
    \\\\ F = \frac{(SSR _r -SSR _{ur})/q}{SSR _{ur}/(n - k -1)} = \frac{(1.880 - 1.822)/4}{1.822/(88 - 4 -1)} \approx .661
    \\\\ F \sim F _{4,83} \Rightarrow c _0.05 = 2.50 \Rightarrow H _0 \text{  cannot be rejected}
    \end{array}
$$

-   The F-test works for general multiple linear hypotheses  
&emsp;Fæ£€éªŒé€‚ç”¨äºä¸€èˆ¬å¤šé‡çº¿æ€§å‡è®¾

-   For all tests and confidence intervals, validity of assumptions MLR.1 â€“MLR.6 has been assumed. Tests may be invalid otherwise.  
&emsp;å¯¹äºæ‰€æœ‰æµ‹è¯•å’Œç½®ä¿¡åŒºé—´ï¼Œå‡è®¾MLR.1â€“MLR.6çš„æœ‰æ•ˆæ€§å·²è¢«å‡å®šã€‚å¦åˆ™æµ‹è¯•å¯èƒ½æ— æ•ˆã€‚

-   <u>**F- and t-statistics**</u>  
&emsp;<u>**F-å’Œt-ç»Ÿè®¡**</u>

    -   If \\( q=1, F = t ^2\\)

    -   The *t-test* is more transparent and allows one-sided alternatives  
    &emsp;tæ£€éªŒæ›´ä¸ºé€æ˜ï¼Œå…è®¸å•è¾¹é€‰æ‹©

-   **\\(\underline{R ^2 \text{ form of the } F-statistic}\\)**

    -   Assume that the same dependent variable is used in two regressions  
    &emsp;å‡è®¾åœ¨ä¸¤ä¸ªå›å½’ä¸­ä½¿ç”¨ç›¸åŒçš„å› å˜é‡

    -   \\( SSR _r = (1-R _r^2)SST \text{,   } SSR _{ur} = (1 - R _{ur}^2) SST\\)

$$
    F = \frac{(R _{ur}^2 - R _r^2)/q}{(1-R _{ur}^2)/(n - k -1)}
$$

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
