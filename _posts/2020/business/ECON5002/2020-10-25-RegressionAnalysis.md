---
layout: post
title: Basic Econometrics - Unit 2 Regression Analysis Estimation
subtitle: åŸºç¡€è®¡é‡ç»æµå­¦ï¼ˆäºŒï¼‰å›å½’åˆ†æä¼°è®¡
category: money
tags: [ECON5002]
---


translated by damien from Marco Avarucci

#  Basic Econometrics - Unit 2 Regression Analysis Estimation (Simple Regression)

# Video 1: Conditional Expectation and Prediction  
&emsp;è§†é¢‘1ï¼šæ¡ä»¶æœŸæœ›ä¸é¢„æµ‹

##  The Distribution of Wages  
&emsp;å·¥èµ„åˆ†é…

- Suppose that we are interested in wage rates in the U.S.  
&emsp;å‡è®¾æˆ‘ä»¬å¯¹ç¾å›½çš„å·¥èµ„ç‡æ„Ÿå…´è¶£ã€‚

- We can view the *wage* of an individual worker as a random variable wage with distribution \\( F(z) = Pr (wage \le z) \\):  
&emsp;æˆ‘ä»¬å¯ä»¥å°†å•ä¸ªå·¥äººçš„*wage*è§†ä¸ºéšæœºå˜é‡å·¥èµ„ï¼Œå…¶åˆ†å¸ƒ \\( F(z) = Pr (wage \le z) \\):

- By saying that a person's wage is random mean that we do not know her wage before it is measured.  
&emsp;å¦‚æœè¯´ä¸€ä¸ªäººçš„å·¥èµ„æ˜¯éšæœºçš„ï¼Œé‚£å°±æ„å‘³ç€æˆ‘ä»¬åœ¨è¡¡é‡ä¹‹å‰ä¸çŸ¥é“å¥¹çš„å·¥èµ„ã€‚

- The observed wage rates are treated as realizations from the same distribution \\( F(z)\\).  
&emsp;è§‚å¯Ÿåˆ°çš„å·¥èµ„ç‡è¢«è§†ä¸ºæ¥è‡ªç›¸åŒåˆ†å¸ƒçš„å®ç°\\( F(z)\\)ã€‚

![]({{site.url}}/assets/images/2020/ECON5002/wageDis.png "Figure 1.1: Wage Distribution and Densities. All Full-time U.S. Workers (Hansen (2000), Chapter 2).")

We will pretend the c.d.f and p.d.f.s refer to the **Population**!

###  Data

- Data: https://www.ssc.wisc.edu/~bhansen/econometrics/

- Description: https://www.ssc.wisc.edu/~bhansen/econometrics/cps09mar_description.pdf

- The distribution and the density are estimated from a sample of 50,742 full-time non-military wage-earners reported in the March 2009 Current Population Survey.  
&emsp;è¯¥åˆ†å¸ƒå’Œå¯†åº¦æ˜¯æ ¹æ®2009å¹´3æœˆæœ¬æ¬¡äººå£è°ƒæŸ¥æŠ¥å‘Šçš„50742åå…¨èŒéå†›äººå·¥è–ªé˜¶å±‚æ ·æœ¬ä¼°è®¡çš„ã€‚

- The wage rate is constructed as annual individual wage and salary earning divided by hours worked.  
&emsp;å·¥èµ„ç‡æ˜¯æŒ‡ä¸ªäººå¹´å·¥èµ„å’Œå·¥èµ„æ”¶å…¥é™¤ä»¥å·¥ä½œæ—¶é—´ã€‚

- The measures of central tendency describe some features of the distribution.  
&emsp;é›†ä¸­è¶‹åŠ¿çš„åº¦é‡æè¿°äº†åˆ†å¸ƒçš„ä¸€äº›ç‰¹å¾ã€‚

- The density is peaked around $15, and most of the probability mass lie between $10 and $40.  
&emsp;å¯†åº¦å³°å€¼åœ¨15ç¾å…ƒå·¦å³ï¼Œå¤§éƒ¨åˆ†æ¦‚ç‡è´¨é‡åœ¨10ç¾å…ƒåˆ°40ç¾å…ƒä¹‹é—´ã€‚

- The median is $19.23, the mean is $23.90.  
&emsp;ä¸­ä½æ•°æ˜¯19.23ç¾å…ƒï¼Œå¹³å‡å€¼æ˜¯23.90ç¾å…ƒã€‚

- Right skewness and thick tails explain why Mean > Median (64% of workers earn less than $23.90).  
&emsp;å³åå’Œåšå°¾è§£é‡Šäº†ä¸ºä»€ä¹ˆå‡å€¼>ä¸­ä½æ•°ï¼ˆ64%çš„å·¥äººæ”¶å…¥ä½äº23.90ç¾å…ƒï¼‰ã€‚

- The expectation is not robust(1: The expectation is sensitive to perturbation in the tails of the distribution) but it is mathematically convenient.  
&emsp;æœŸæœ›ä¸æ˜¯é²æ£’çš„ï¼ˆ1ï¼šæœŸæœ›å¯¹åˆ†å¸ƒå°¾éƒ¨çš„æ‰°åŠ¨å¾ˆæ•æ„Ÿï¼‰ï¼Œä½†åœ¨æ•°å­¦ä¸Šå¾ˆæ–¹ä¾¿ã€‚

- It is useful to transform the data: the density of ln(*wage*) is much less skewed and fat-tailed, and the mean is a much "better" measure of central tendency of the distribution.  
&emsp;è½¬æ¢æ•°æ®æ˜¯æœ‰ç”¨çš„ï¼šlnï¼ˆ*wage*ï¼‰çš„å¯†åº¦ä¸é‚£ä¹ˆåæ–œå’Œåšå°¾ï¼Œå¹³å‡å€¼æ˜¯åˆ†å¸ƒä¸­å¿ƒè¶‹åŠ¿çš„ä¸€ä¸ªæ›´å¥½çš„åº¦é‡ã€‚

### Conditional Expectation  
&emsp;æ¡ä»¶æœŸæœ›

- Consider the density of ln(*wage*) saw in Fig 1.1(c).  
&emsp;è€ƒè™‘å›¾1.1ï¼ˆcï¼‰æ‰€ç¤ºçš„lnï¼ˆ*wage*ï¼‰å¯†åº¦ã€‚

- Does the wage distribution vary across sub-populations?  
&emsp;å·¥èµ„åˆ†å¸ƒåœ¨ä¸åŒçš„äºšäººç¾¤ä¸­æœ‰å·®å¼‚å—ï¼Ÿ

- It is reasonable to expect that more educated people tend to earn more than less educated people.  
&emsp;æˆ‘ä»¬æœ‰ç†ç”±è®¤ä¸ºï¼Œå—æ•™è‚²ç¨‹åº¦é«˜çš„äººå¾€å¾€æ¯”å—æ•™è‚²ç¨‹åº¦ä½çš„äººæŒ£å¾—å¤šã€‚

- This will not hold for each individual, but it is likely to be true on average.  
&emsp;è¿™å¹¶ä¸é€‚ç”¨äºæ¯ä¸ªäººï¼Œä½†å¹³å‡è€Œè¨€ï¼Œè¿™å¾ˆå¯èƒ½æ˜¯æ­£ç¡®çš„ã€‚

- The conditional expectation is a measure of central tendency for the conditional distribution.  
&emsp;æ¡ä»¶æœŸæœ›æ˜¯æ¡ä»¶åˆ†å¸ƒä¸­å¿ƒè¶‹åŠ¿çš„åº¦é‡ã€‚

![]({{site.url}}/assets/images/2020/ECON5002/CEF.png "Figure 1.2: Conditional Expectation Function (CEF)")

\\( E [ ln(wage) \| education ] \\) is a function of education.  
&emsp;ä¸Šå¼æ˜¯ä¸€ä¸ªæ•™è‚²çš„å‡½æ•°ã€‚

Write *lwage* for ln(*wage*) and *educ* for *education*.  
&emsp;å°†*ln(wage)*å†™ä½œ*lwage*ä¸”å°†*education*å†™ä½œ*educ*ã€‚

A high school graduate has *education* = 12  
&emsp;ä¸€ä¸ªé«˜ä¸­æ¯•ä¸šç”Ÿçš„ *education* = 12.

$$
  E(lwage | educ = 12) = 2.71
$$

A professional degree (medical, law or PhD) has *education* = 20:  
&emsp;ä¸€ä¸ªèŒä¸šå­¦ä½ï¼ˆåŒ»å­¦ï¼Œæ³•å¾‹æˆ–è€…åšå£«ï¼‰çš„*education* = 20ï¼š

$$
  E [lwage | educ = 20] = 3.69
$$

![]({{site.url}}/assets/images/2020/ECON5002/condDens.png "Figure 1.3: Conditional densities")

$$
  f(lwage | educ = n), \ n = 12\text{ (dotted) and } n = 20 \text{solid}
$$

Remarks  
&emsp;å¤‡æ³¨

- Does education causes earnings to increase?  
&emsp;æ•™è‚²ä¼šå¯¼è‡´æ”¶å…¥å¢åŠ å—ï¼Ÿ

- Even without answering this question, it seems that education predicts earnings.  
&emsp;å³ä½¿ä¸å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæ•™è‚²ä¼¼ä¹ä¹Ÿèƒ½é¢„æµ‹æ”¶å…¥ã€‚

- We predict when we say in advance, foretell, what is the new or future observation.  
&emsp;å½“æˆ‘ä»¬é¢„å…ˆè¯´ï¼Œé¢„è¨€ï¼Œä»€ä¹ˆæ˜¯æ–°çš„æˆ–æœªæ¥çš„è§‚å¯Ÿæ—¶ï¼Œæˆ‘ä»¬å°±é¢„æµ‹ã€‚

- In Fig 1.2 we used the CEF(educ) to predict lwage.  
&emsp;åœ¨å›¾1.2ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨CEFï¼ˆeducï¼‰é¢„æµ‹lwageã€‚

- Can we find a better predictor?  
&emsp;æˆ‘ä»¬èƒ½æ‰¾åˆ°æ›´å¥½çš„é¢„æµ‹å™¨å—ï¼Ÿ

### Best Predictor  
&emsp;æœ€ä½³é¢„æµ‹å€¼

- Consider the problem of predicting a random variable *y* given a random variable *x*.  
&emsp;å‡è®¾ä¸€ä¸ªéšæœºå˜é‡*x*ï¼Œè€ƒè™‘é¢„æµ‹ä¸€ä¸ªéšæœºå˜é‡*y*çš„é—®é¢˜ã€‚

- We can write any predictor as *h(x)*.  
&emsp;æˆ‘ä»¬å¯ä»¥æŠŠä»»ä½•é¢„æµ‹å™¨å†™æˆ*h(x)*ã€‚

- The (ex-post) prediction error is *y-h(x)*.  
&emsp;ï¼ˆäº‹åï¼‰é¢„æµ‹è¯¯å·®ä¸º*y-h(x)*ã€‚

- A non-stochastic measure of the prediction error is  
&emsp;æå‡ºäº†é¢„æµ‹è¯¯å·®çš„ééšæœºæµ‹åº¦

$$
  E [ y - h (x)] ^2 
  \tag{1.1} \label{predictionError}
$$

- We define the best predictor(2: in the mean square sense) as the function of x which minimizes \ref{predictionError}.  
&emsp;æˆ‘ä»¬å°†æœ€ä½³é¢„æµ‹å› å­ï¼ˆ2ï¼šå‡æ–¹æ„ä¹‰ä¸Šçš„ï¼‰å®šä¹‰ä¸ºxçš„å‡½æ•°ï¼Œå®ƒä½¿\ref{predictionError}æœ€å°åŒ–ã€‚

- It turns out that the best predictor is \\( E(y \| x) \\).  
&emsp;ç»“æœè¡¨æ˜ï¼Œæœ€å¥½çš„é¢„æµ‹å› å­æ˜¯\\( E(y \| x) \\)ã€‚

- If we set \\( m(x) = E(y \| x) \\) and define \\( u = y - m(x) \\), then, by construction  
&emsp;å¦‚æœæˆ‘ä»¬è®¾ç½®\\( m(x)=E(y \| x)\\) å¹¶å®šä¹‰\\( u = y - m(x)\\)ï¼Œé‚£ä¹ˆï¼Œé€šè¿‡æ„é€ 

$$
  y = m(x) + u \text{, with } E(u | x) = 0
  \tag{1.2} \label{}
$$

- An important special case is when the CEF is linear,  
&emsp;ä¸€ä¸ªé‡è¦çš„ç‰¹ä¾‹æ˜¯å½“CEFä¸ºçº¿æ€§æ—¶ï¼Œ

$$
  m(x) = \beta _0 + \beta _1 x
$$

The linear CEF model is often called **linear regression model**, or regression of *y* on *x*.  
&emsp;çº¿æ€§CEFæ¨¡å‹ç»å¸¸è¢«ç§°ä½œ**çº¿æ€§å›å½’æ¨¡å‹**ï¼Œæˆ–è€…*y*åœ¨*x*ä¸Šçš„å›å½’ã€‚

**Interpreting the CEF**  
&emsp;è§£æCEF


If \\( E(y \| x)\\) is linear, i.e. \\( m(x) = \beta _0 + \beta _1 x \\), then  
&emsp;å¦‚æœä¸Šå¼æˆç«‹ï¼Œé‚£ä¹ˆ

$$
  \beta _0 = m(0).
$$


and \\( \beta _1\\) can be seen as marginal changes in \\( m(x) \\) implied by *a* change in *x*  
&emsp;è€Œä¸”\\( \beta _1\\)å¯ä»¥è¢«çœ‹ä½œæ˜¯åœ¨\\( m(x) \\)ä¸Šçš„è¾¹é™…å˜åŒ–å€¼ï¼Œç”±*a*åœ¨*x*ä¸Šçš„æ”¹å˜å¯ä»¥æ¨æ–­å‡ºæ¥

$$
  \Delta m(x) = m(x + \Delta x) - m(x) = \beta _1 \Delta x \implies \beta _1 = \frac{\Delta m(x)}{\Delta x}
$$

If x is continuous  
&emsp;å¦‚æœxæ˜¯è¿ç»­çš„

$$
  \beta _1 = \lim _{\Delta x \to 0 } \frac{\Delta m(x)}{\Delta x} 
  = \frac{d m(x)}{d x} = m ' (x)
$$

**Best Linear Predictor**  
&emsp;æœ€ä½³çº¿æ€§é¢„æµ‹å€¼

- While \\( m(x) = E(y \| x) \\) is the best predictor among all function of *x*, its functional form can be complicated.  
&emsp;è™½ç„¶\\( m(x) = E(y \| x) \\)æ˜¯*x*æ‰€æœ‰å‡½æ•°ä¸­æœ€å¥½çš„é¢„æµ‹å› å­ï¼Œä½†å®ƒçš„å‡½æ•°å½¢å¼å¯èƒ½å¾ˆå¤æ‚ã€‚

- We can restrict \\( h(x)\\) to be linear, i.e. \\( h(x) = b _0 + b _1 x\\).  
&emsp;æˆ‘ä»¬å¯ä»¥å°†\\( h(x)\\)é™åˆ¶ä¸ºçº¿æ€§ï¼Œå³\\( h(x) = b _0 + b _1 x\\)ã€‚

- The *best linear predictor* (BLP) of *y* given *x* is \\( \beta _0 + \beta _1 x \\), where  
&emsp;ç»™å®š*x*çš„*y*çš„*best linear predictor*ï¼ˆBLPï¼‰æ˜¯\\( \beta _0 + \beta _1 x \\)ï¼Œå…¶ä¸­

$$
  [\beta _0, \beta _1] = \mathop{\arg \max} \limits _{b _0, b _1} E[( y - b _0 - b _1 x) ^2]
  \tag{1.3} \label{blp}
$$

- If the moments exist and \\( Var(x) > 0 \\)  
&emsp;å¦‚æœè¿™ä¸ªæ—¶åˆ»å­˜åœ¨è€Œä¸”\\( Var(x) > 0 \\)

$$
  \beta _0 = E(y) - \beta _1 E(x), \quad \beta _1 = \frac{Cov(x,y)}{Var(x)} 
  \tag{1.4} \label{blp2}
$$

![]({{site.url}}/assets/images/2020/ECON5002/CEFandBLP.png "Figure 1.4: CEF and BLP")


![]({{site.url}}/assets/images/2020/ECON5002/cumuDisFuncEduc.png "Figure 1.5: Cumulative Distribution function of education")


![]({{site.url}}/assets/images/2020/ECON5002/predLwageLinearEduc.png "Figure 1.6: Prediction lwage with a linear function of educ")

### The OLS estimator  
&emsp;æœ€å°äºŒä¹˜æ³•çš„ä¼°è®¡å€¼

Given a data set \\( {(y _i, x _i), i = 1, \cdots, n} \\), the estimator of \\( [\beta _0, \beta _1] \\) is derived as the solution of the *sample* counterpart of \eqref{blp}, i.e.  
&emsp;ç»™å®šä¸Šé¢çš„æ•°æ®é›†ï¼Œå¯¼å‡ºä¼°è®¡é‡\\( [\beta _0, \beta _1] \\)ä½œä¸ºä½œä¸º\eqref{blp}çš„*sample*å¯¹åº”é¡¹çš„è§£ï¼Œå³

$$
  [ \hat{\beta} _0, \hat{\beta} _1] \mathop{\arg \min} \limits _{b _0, b _1} \frac{1}{n} \sum _{i =1}^n u _i^2 (b _0 , b _1) 
  \tag{1.5} \label{ols}
$$

where \\( u _i (b _0, b _1) = y _i - b _0 - b _1 x _i \\)

The solution of \eqref{ols} is the OLS estimator  
&emsp;ä¸Šé¢å¼\eqref{ols}çš„è§£ä¾¿æ˜¯æœ€å°äºŒä¹˜æ³•çš„ä¼°è®¡å€¼

$$
  \begin{cases}
  \hat{\beta} _0  = \overline{y} - \hat{\beta} _1 \overline{x},
  \\\\
  \hat{\beta} _1 = \frac{\sum _{i=1}^n (x _i - \overline{x})(y _i - \overline{y})}{\sum _{i=1}^n (x _i - \overline{x}) ^2}
  \end{cases}
$$

where \\( \overline{y} = n ^{-1} \sum _{i=1}^n y _i \text{ and } \overline{x} = n ^{-1} \sum _{i =1 }^n x _i \\). 

Note that  
&emsp;æ³¨æ„

$$
  \hat{\beta} _1 = \frac{ (n-1) ^{-1} \sum _{i=1}^n (x _i - \overline{x})(y _i - \overline{y})}{ (n-1) ^{-1} \sum _{i=1}^n (x _i - \overline{x}) ^2} 
  = \frac{\widehat{Cov} (x,y)}{\widehat{Var} (x)}
$$

Compare with \eqref{blp2}.  
&emsp;ä¸\eqref{blp2}æ¯”è¾ƒã€‚

### Takeaway  
&emsp;è¯¾å¤–

- The CEF is useful to model the relationship between *y* and *x*  
&emsp;æ¡ä»¶æœŸæœ›æ–¹ç¨‹CEFæ˜¯æœ‰åŠ©äºå»ºæ¨¡*y*ä¸*x*ä¹‹é—´çš„å…³ç³»ã€‚

$$
  y = E(y | x) +u
$$

- The simple linear regression model  
&emsp;ç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹

$$
  y = \beta _0 + \beta _1 + u
$$

has been derived as the solution of a liner prediction problem.  
&emsp;è¢«å¯¼å‡ºä½œä¸ºä¸€ä¸ªçº¿æ€§é¢„æµ‹é—®é¢˜çš„è§£ã€‚

- \\( [\beta _0, \beta _1] \\) is called OLS estimator because it solves a least square problem.  
&emsp;ä¸Šé¢çš„è§£è¢«ç§°ä½œæœ€å°äºŒä¹˜æ³•OLSçš„ä¼°è®¡å€¼å› ä¸ºå®ƒè§£å†³äº†æœ€å°äºŒä¹˜é—®é¢˜ã€‚

## Video 2: Interpretation of the simple regression model  
&emsp;è§†é¢‘2ï¼šç®€å•å›å½’æ¨¡å‹çš„è§£é‡Š

Consider the simple regression model:  
&emsp;è€ƒè™‘è¿™ä¸ªç®€å•çš„å›å½’æ¨¡å‹ï¼š

$$ \tag{2.1} y =  \beta _0 + \beta _1 x + u  \label{simpleRegression1} $$

If the factors in \\( u \\) are held fixed when \\( x \\) changes by \\( \Delta x \\)  
&emsp;å¦‚æœå½“\\( x \\) éšç€\\( \Delta x \\)è€Œæ”¹å˜æ—¶ï¼Œ\\( u \\)ä¸­çš„å› å­ä¿æŒä¸å˜

$$ (y+\Delta y) = \beta _0 + \beta _1 (x + \Delta x) + u \tag{2.2} \label{simpleRegression2} $$

Substracting \eqref{simpleRegression1} from \eqref{simpleRegression2} we got  
&emsp;ä»\eqref{simpleRegression2}å‡å»\eqref{simpleRegression1}ï¼Œæˆ‘ä»¬å¾—åˆ°

$$ \Delta y = \beta _1 \Delta x \text{, if } \Delta u = 0 $$

Suppose  
&emsp;è®¾æƒ³

$$ wage = -16.45 + 2.9 educ + u$$

An additional year of education increase the wage by $2.9, if \\( u \\) doesn't change when \\( educ \\) changes, i.e.  
&emsp;å¦‚æœå½“\\( educ \\)æ”¹å˜æ—¶\\( u \\)ä¸å˜ï¼Œæ¯å¢åŠ ä¸€å¹´çš„æ•™è‚²å¢åŠ $2.9çš„å·¥èµ„ã€‚

$$ \Delta wage = 2.9 \Delta educ \text{, if } \Delta u = 0. $$

Suppose that  
&emsp;è®¾æƒ³

$$ lwage = 1.44 + 0.11 educ + u $$

What is the effect on wage of an additional year of education?  
&emsp;å¢åŠ ä¸€å¹´çš„æ•™è‚²å¯¹å·¥èµ„çš„å½±å“æ˜¯ä»€ä¹ˆï¼Ÿ

$$
  \Delta ln{wage} 
  = ln(wage+\Delta wage) - ln(wage)
  = ln \left( \frac{wage+\Delta wage}{wage}\right)
$$

If \\(\epsilon = \Delta wage / wage \\) is small  
&emsp;å¦‚æœ\\(\epsilon = \Delta wage / wage \\) æ˜¯å°çš„

$$
  \Delta ln(wage) = ln(1+\epsilon) \approx \epsilon
$$

If \\( \Delta ln (wage) = \beta _1\\), then,  
&emsp;å¦‚æœ\\( \Delta ln (wage) = \beta _1\\)ï¼Œé‚£ä¹ˆï¼Œ

$$
  \beta _1 
  \approx \frac{\Delta wage} {wage} 
  \implies \\% \ \Delta wage 
  \approx 100 \beta _1
$$

Another year of education increases the predicted wage by 11%.  
&emsp;ä¸€å¹´çš„æ•™è‚²å¢åŠ äº†11%çš„é¢„æœŸå·¥èµ„ã€‚

### Remarks  
&emsp;å¤‡æ³¨ï¼š

- \\( \beta _1 \\) measures the effect of  \\(x\\) on  \\(y\\) holding the other factors in \\(u\\) fixed.  
&emsp;\\( \beta _1 \\) æµ‹é‡xå¯¹yçš„å½±å“ä¿æŒuä¸­çš„å…¶ä»–å› ç´ ä¸å˜ã€‚

- How can we learn the ceteris paribus effect of  \\(x\\)  on \\(y\\), when we are ignoring all the other factors?  
&emsp;å½“æˆ‘ä»¬å¿½è§†æ‰€æœ‰å…¶ä»–å› ç´ æ—¶ï¼Œæˆ‘ä»¬æ€ä¹ˆèƒ½ä»xå¯¹yçš„ç›¸åŒæ¡ä»¶ä¸­å­¦ä¹ ï¼Ÿ

-  If we assume \\( E(u|x) = 0 \\), then \\( \beta _1 \Delta x\\)  tell us how the average value of \\(y\\) changes with  \\(x\\) .  
An additional year of education increases the predicted wage by 11% on average, not for every worker.  
&emsp;å¦‚æœæˆ‘ä»¬å‡è®¾\\( E(u|x) = 0 \\)ï¼Œé‚£ä¹ˆ\\( \beta _1 \Delta x\\)å‘Šè¯‰æˆ‘ä»¬yçš„å¹³å‡å€¼æ˜¯å¦‚ä½•éšç€xè€Œæ”¹å˜çš„ã€‚å¢åŠ ä¸€å¹´çš„æ•™è‚²ä¼šå¹³å‡å¢åŠ 11%çš„é¢„æœŸå·¥èµ„ï¼Œè€Œä¸æ˜¯å¯¹äºæ¯ä¸ªå‘˜å·¥ã€‚

### Takeaway  
&emsp;è¯¾å¤–

- If we assume that \\( E(u|x) = 0 \\), the slope \\( \beta _1 \\) can be interpreted as the average predicted change in y when x increases by one unit.  
It can be also interpreted as the predicted change in y, if we are ready to assume that  \\(u\\)  remains fixed when \\(x\\) changes.  
&emsp;å¦‚æœæˆ‘ä»¬å‡è®¾\\( E(u|x) = 0\\)ï¼Œæ–œç‡\\( \beta _1 \\)èƒ½è¢«è§£é‡Šä¸ºå½“xå¢é•¿ä¸€ä¸ªå•ä½æ—¶ï¼Œyçš„å¹³å‡é¢„æœŸå˜åŒ–é‡ã€‚

- The log transformation of the variables allow for constant (semi)-elasticity models.  
&emsp;å˜é‡çš„å¯¹æ•°å˜æ¢å…è®¸æ’å®šçš„ï¼ˆåŠï¼‰å¼¹æ€§æ¨¡å‹

## Video 3: Mechanics of Ordinary Least Squares  
&emsp;è§†é¢‘3ï¼šæœ€å°äºŒä¹˜æ³•çš„æœºåˆ¶

### Obtaining the OLS estimates  
&emsp;è·å–æœ€å°äºŒä¹˜æ³•çš„ä¼°è®¡å€¼

Given that data set \\( \lbrace (lwage_i, educ_i); i = 1, \cdots, n \rbrace \\), suppose that we are interested in the relationship between wages and education.  
&emsp;è€ƒè™‘åˆ°æ•°æ®é›†\\( \lbrace (lwage_i, educ_i); i = 1, \cdots, n \rbrace \\)ï¼Œå‡è®¾æˆ‘ä»¬å¯¹å·¥èµ„å’Œæ•™è‚²ä¹‹é—´çš„å…³ç³»æ„Ÿå…´è¶£ã€‚

If we assume  
&emsp;å¦‚æœæˆ‘ä»¬å‡è®¾

$$ lwage_i \approx b_0 + b_1 educ_i $$

then we are interested in b0 and b1. 
&emsp;é‚£ä¹ˆæˆ‘ä»¬å¯¹b0å’Œb1æ„Ÿå…´è¶£ã€‚

![]({{site.url}}/assets/images/2020/ECON5002/OLSestimates.png "Figure 3.1: How do we pick up the 'optimal' line?")

Given a data set \\( \lbrace (y _i, x _i) i = 1, \cdots, n \rbrace \\), for each observation, let  
&emsp;è€ƒè™‘åˆ°ä¸€ä¸ªæ•°æ®é›†\\( \lbrace (y _i, x _i) i = 1, \cdots, n \rbrace \\)ï¼Œå¯¹æ¯ä¸€ä¸ªè§‚æµ‹å€¼ï¼Œè®©

$$
  u _i (b _0, b _1) = y _i - (b _0 + b_1 x _i) 
  \tag{3.1} 
$$

and define the **ordinary least-square estimator** as  
&emsp;ä¸”å®šä¹‰**æ™®é€šæœ€å°äºŒä¹˜ä¼°è®¡é‡**ä¸º

$$
  [\hat{\beta} _0, \hat{\beta} _1] = \mathop{\arg\min}\limits _{b _0, b _1} \hat{S} (b _0, b _1)
  \tag{3.2} \label{OLSestimator}
$$

where  
&emsp;åœ¨è¿™é‡Œ

$$
  \hat{S} (b _0, b _1) \sum\limits _{i=1} ^{n} u ^2 _i (b _0, b _1)
  \tag{3.3}
$$

### The OLS estimator  
&emsp;æœ€å°äºŒä¹˜ä¼°è®¡é‡

The solution of \eqref{OLSestimator} is  
&emsp;å…¬å¼\eqref{OLSestimator}çš„è§£æ³•æ˜¯

$$
  \left \lbrace \begin{array}{l} 
  \hat{\beta} _0 = \overline{y} -\hat{\beta} _1 \overline{x}
  \\\\ \hat{\beta} _1 = \frac 
    {\sum _{i=1} ^n (x _i - \overline{x}) (y _i - \overline{y})}
    {\sum _{i=1} ^n (x _i - \overline{x})^2}
  \end{array} \right.
$$

assuming that \\( SST _x = \sum _{i=1} ^n (x _i - \overline{x})^2 > 0. \\)  
&emsp;å‡è®¾ä»¥ä¸Šå…¬å¼

The fitted or predicted values, \\( \hat{y} _i \\), is defined by the **Sample Regression Function** (SRF)  
&emsp;æ‹Ÿåˆå€¼æˆ–é¢„æµ‹å€¼\\( \hat{y} _i \\)ç”±æ ·æœ¬å›å½’å‡½æ•°ï¼ˆSRFï¼‰å®šä¹‰

$$
  \hat{y} _i = \hat{\beta} _0 + \hat{\beta} _1 x _i
$$

The deviations from the SRF (or fitted regression line)  
&emsp;ä¸SRFï¼ˆæˆ–æ‹Ÿåˆå›å½’çº¿ï¼‰çš„åå·®

$$
  \hat{u} _i = y _i - \hat{y} _i, \  i = 1,\cdots, n
$$

are called **residuals**.  
&emsp;è¢«å«åš**æ®‹å·®**

Remark: We defined \\(   u _i (b _0, b _1) = y _i - (b _0 + b_1 x _i)  \\). Then, \\( \hat{u} _i =  u _i( \hat{\beta} _0, \hat{\beta} _1) \\).  
&emsp;å¤‡æ³¨ï¼šæˆ‘ä»¬å®šä¹‰\\(   u _i (b _0, b _1) = y _i - (b _0 + b_1 x _i)  \\)ï¼Œé‚£ä¹ˆï¼Œ\\( \hat{u} _i =  u _i( \hat{\beta} _0, \hat{\beta} _1) \\)ã€‚


![]({{site.url}}/assets/images/2020/ECON5002/OLSestimates2.png 'Figure 3.2: The sample regression function')

### Algebraic Properties of OLS statistics  
&emsp;æœ€å°äºŒä¹˜æ³•ç»Ÿè®¡é‡çš„ä»£æ•°æ€§è´¨

Recall equation \eqref{OLSestimator}  
&emsp;å›å¿†ç­‰å¼\eqref{OLSestimator}


<!-- inline equation:
\\(
   f(x) =  \frac{1}{2} x^2 = \dfrac{1}{2} x^2 = \tfrac{1}{2} x^2
\\)

display equation:  
$$
  f(x) =  \frac{1}{2} x^2 = \dfrac{1}{2} x^2 = \tfrac{1}{2} x^2
$$ -->


<!-- $$
    \left.\frac{dy}{dx}\right|_{x=0}
$$ -->

$$
  [\hat{\beta} _0, \hat{\beta} _1] 
  = \mathop{\arg\min}\limits _{b _0, b _1} \hat{S} (b _0, b _1)
  = \mathop{\arg\min}\limits _{b _0, b _1} \sum _{i=1}^{n} u _i ^2 (b _0, b _1)
$$

First order conditions:  
&emsp;ä¸€é˜¶æ¡ä»¶ï¼š

$$
  \begin{array}{l}
    \left .\frac{\partial{\hat{S}(b _0, b _1)}}{\partial{b _0}} \right| 
      _{ \begin{array}{l} 
        b_0 = \hat{\beta} _0 
        \\\\ b_1 = \hat{\beta} _1 
        \end{array}}
    = \sum _{i=1}^n u _i \left(\hat{\beta} _0, \hat{\beta} _1 \right)
    = \sum _{i=1}^n \hat{u} _i 
    = 0
    \\\\
    \left .\frac{\partial{\hat{S}(b _0, b _1)}}{\partial{b _0}} \right| 
      _{ \begin{array}{l} 
        b_0 = \hat{\beta} _0 
        \\\\ b_1 = \hat{\beta} _1 
        \end{array}}
    = \sum _{i=1}^n u _i \left(\hat{\beta} _0, \hat{\beta} _1 \right) x _i
    = \sum _{i=1}^n \hat{u} _i x _i 
    = 0
  \end{array}
  \tag{3.4} \label{FOC}
$$

1. Deviations from regression line sum up to zero  
&emsp;ä¸€ï¼Œä¸å›å½’çº¿çš„åå·®æ€»å’Œä¸ºé›¶

$$
  \sum _{i=1}^n \hat{u} _i = 0
  \tag{3.5}
$$

2. The sample correlation between deviations and regressors is zero  
&emsp;äºŒï¼Œåå·®å’Œå›å½’ç³»æ•°ä¹‹é—´çš„æ ·æœ¬ç›¸å…³æ€§ä¸ºé›¶

$$
  \sum _{i=1}^n \hat{u} _i x _i = 0
  \tag{3.6}
$$

3. Sample averages of y and x lie on the regression line  
&emsp;ä¸‰ï¼Œyå’Œxçš„æ ·æœ¬å¹³å‡å€¼ä½äºå›å½’çº¿ä¸Š

$$
  \overline{y} = \hat{\beta} _0 + \hat{\beta} _1 \overline{x}
  \tag{3.7}
$$

4. Properties 1 and 2 imply  
&emsp;å››ï¼Œæ€§è´¨1å’Œ2æ„å‘³ç€

$$
  \sum _{i=1}^n \hat{y} _i \hat{u} _i   = 0 
  \tag{3.8}
$$

5. Properties 1 implies  
&emsp;äº”ï¼Œæ€§è´¨1æ„å‘³ç€

$$
  \overline{y} 
  = \frac{1}{n} \sum _{i=1}^n y _i
  = \frac{1}{n} \sum _{i=1}^n \hat{y} _i .
  \tag{3.9}
$$

Measures of Variation  
&emsp;å˜åŒ–é‡

- Total Sum of Squares (total sample variation of the \\(y_i\\) )  
&emsp;æ€»å¹³æ–¹å’Œï¼ˆ\\(y_i\\)çš„æ€»æ ·æœ¬å˜åŒ–é‡ï¼‰

$$
  \text{SST} 
  = \sum _{i=1}^n ( y _i - \overline{y}) ^2
$$

- Explained Sum of Squares (total sample variation of the \\(\hat{y}_i\\) )  
&emsp;è§£é‡Šå¹³æ–¹å’Œï¼ˆ\\(\hat{y}_i\\)çš„æ€»æ ·æœ¬å˜åŒ–é‡ï¼‰


$$
  \text{SSE} 
  = \sum _{i=1}^n ( \hat{y} _i - \overline{y}) ^2
  = \sum  _{i=1}^n \hat{\beta} ^2 (x _i - \overline{x}) ^2
$$

- Sum of Squared Residuals (total sample variation of the \\(\hat{u}_i\\))  
&emsp;æ®‹å·®å¹³æ–¹å’Œï¼ˆ\\(\hat{u}_i\\)çš„æ€»æ ·æœ¬å˜åŒ–é‡ï¼‰

$$
  \text{SSR} 
  = \sum _{i=1}^n \hat{u}_i ^2.
$$

- Decomposition of total variation  
&emsp;æ€»å˜å·®åˆ†è§£

$$
  \text{SST} = \text{SSE} +\text{SSR}
  \tag{3.10}
$$

i.e.

Total Variation = Explained Variation + Unexplained Variation  
&emsp;æ€»åå·®=è§£é‡Šåå·®+ä¸ºè§£é‡Šåå·®

- Goodness offit measure(R-squared)  
&emsp;æ‹Ÿåˆä¼˜åº¦ï¼ˆRå¹³æ–¹ï¼‰

$$
  R ^2 = \frac{SSE}{SST} = 1- \frac{SSR}{SST},\ 0 \le R ^2 \le 1
$$

The \\(R ^2\\) measure the fraction of the total variation explained by the regression.  
&emsp;\\(R ^2\\)åº¦é‡å›å½’è§£é‡Šçš„æ€»å˜å¼‚çš„åˆ†æ•°ã€‚

### Takeaway  
&emsp;è¯¾å¤–

- The algebraic properties of OLS follow from the first order conditions in (4).  
&emsp;OLSçš„ä»£æ•°æ€§è´¨éµå¾ªï¼ˆ4ï¼‰ä¸­çš„ä¸€é˜¶æ¡ä»¶ã€‚

- To compute the estimators, we only assumed variation in the \\(x _i, i = 1 \cdots n\\)  
&emsp;ä¸ºäº†è®¡ç®—ä¼°è®¡é‡ï¼Œæˆ‘ä»¬åªå‡è®¾\\(x _i, i = 1 \cdots n\\) ä¸­çš„å˜é‡

- Learning how to handle OLS as a computational tool will be (hopefully) helpful with the usage and the interpretation of OLS as an estimator.  
&emsp;å­¦ä¹ å¦‚ä½•å°†OLSä½œä¸ºä¸€ç§è®¡ç®—å·¥å…·ï¼Œå°†ï¼ˆå¸Œæœ›ï¼‰æœ‰åŠ©äºOLSä½œä¸ºä¼°è®¡é‡çš„ä½¿ç”¨å’Œè§£é‡Šã€‚

## Video4: Expected Value and Variance of the OLS Estimators  
&emsp;è§†é¢‘4ï¼šæœ€å°äºŒä¹˜ä¼°è®¡é‡çš„é¢„æœŸå€¼ä¸æ–¹å·®

### Unbiasedness of OLS  
&emsp;æœ€å°äºŒä¹˜æ³•çš„æ— åæ€§

**Assumption SLR.2 (random sampling)**  
&emsp;**å‡è®¾SLR.2ï¼ˆéšæœºæŠ½æ ·ï¼‰**

The random variables \\( \lbrace (y _1, x _1), \cdots , (y _i, x _i), \cdots , (y _n; x _n) \rbrace \\) are independent and identically distributed (i.i.d.)  
&emsp;éšæœºå˜é‡\\( \lbrace (y _1, x _1), \cdots , (y _i, x _i), \cdots , (y _n; x _n) \rbrace \\)æ˜¯ç‹¬ç«‹çš„ã€åŒåˆ†å¸ƒçš„ï¼ˆi.i.d.ï¼‰

**Assumption SLR.1 (linearity in the parameters)**  
&emsp;**å‡è®¾SLR.1ï¼ˆå‚æ•°çº¿æ€§ï¼‰**

The random variables \\( (y _i, x _i) \\) satisfy the linear regression equation  
&emsp;éšæœºå˜é‡\\( (y _i, x _i) \\)æ»¡è¶³çº¿æ€§å›å½’æ–¹ç¨‹

$$
  y _i = \beta _0 + \beta _1 x _i + u _i
  \tag{4.1}
$$

where \\(\beta _0\\) and \\(\beta _1\\) are the (unknown) population *intercept* and *slope* parameters.  
&emsp;å…¶ä¸­\\(\beta _0\\) å’Œ\\(\beta _1\\)æ˜¯ï¼ˆæœªçŸ¥ï¼‰æ€»ä½“*æˆªè·*å’Œ*æ–œç‡*å‚æ•°ã€‚

**Assumption SLR.3 (sample variation in x)**  
**å‡è®¾SLR.3ï¼ˆxä¸­çš„æ ·æœ¬å˜åŒ–ï¼‰**

(i) \\( 0 < Var(x _i) < 1 \\) .

(ii) The sample outcome \\( \lbrace x _1, \cdots , x _n \rbrace \\) are not all the same value.  
&emsp;ç¤ºä¾‹ç»“æœ\\( \lbrace x _1, \cdots , x _n \rbrace \\)çš„å€¼ä¸å®Œå…¨ç›¸åŒã€‚

Recall that  
&emsp;å›å¿†

$$
   \hat{\beta} _1 = \frac 
    {\sum _{i=1} ^n (x _i - \overline{x}) (y _i - \overline{y})}
    {\sum _{i=1} ^n (x _i - \overline{x})^2}
$$

![]({{site.url}}/assets/images/2020/ECON5002/constantRegressor.png 'Figure 4.1: Constant regressor.')

**Assumption SLR.4 (Zero Conditional Mean)**  
&emsp;**å‡è®¾SLR.4ï¼ˆé›¶æ¡ä»¶å¹³å‡å€¼ï¼‰**

$$
  E(u _i | x _i) = 0 
$$

- \\( E(U _i) = E[E(u _i \| x _i)] =0 \\).

- Assumption SLR.2 and SLR.4 imply that  
&emsp;å‡è®¾SLR.2å’ŒSLR.4æ„å‘³ç€

$$
  E(y _i | x _i, \cdots , x _n) = E(y _i | x _i) = \beta _0 + \beta _1 x _i 
$$

&emsp;&emsp;that is the **(Population) Regression Function** is linear.  
&emsp;å³**ï¼ˆæ€»ä½“ï¼‰å›å½’å‡½æ•°**æ˜¯çº¿æ€§çš„ã€‚

- If \\(y = log(wage)\\), \\(x = education\\) and \\(u = innate ability\\), assumption SLR.4 implies that the average level of innate ability does not depend on education.  
&emsp;å¦‚æœ\\(y = log(wage)\\)ã€\\(x = education\\) å’Œ \\(u = innate ability\\)ï¼Œå‡è®¾SLR.4æ„å‘³ç€å…ˆå¤©èƒ½åŠ›çš„å¹³å‡æ°´å¹³ä¸ä¾èµ–äºæ•™è‚²ã€‚

**Theorem 2.1. Unbiasedness of OLS**  
&emsp;**å®šç†2.1. OLSçš„æ— åæ€§**

Under Assumption SLR.1-SLR.4  
&emsp;åœ¨å‡è®¾SLR.1-SLR.4ä¸‹

$$
  E(\hat{\beta} _0) =\beta _0, \text{ and }  E(\hat{\beta} _1) =\beta _1 . 
$$

### Variances of the OLS estimators  
&emsp;OLSä¼°è®¡é‡çš„æ–¹å·®

**Assumption SLR.5 (homoskedasticity)**  
&emsp;**å‡è®¾SLR.5ï¼ˆåŒæ„æ€§ï¼‰**

$$
  Var(u _i | x _i) = \sigma ^2
$$

$$
  Var(u _i | x _i) = E(u _i^2  | x _i) - [E(u _i | x _i)] ^2 = E(u _i^2  | x _i)
$$

which means  
&emsp;ä¹Ÿæ„å‘³ç€

$$
  Var(u _i) = E(u _i^2) = E[E(u _i^2  | x _i)] = \sigma ^2 = Var(u _i | x _i).
$$

![]({{site.url}}/assets/images/2020/ECON5002/Homoskedasticity.png 'Figure 4.2: Homoskedasticity')

![]({{site.url}}/assets/images/2020/ECON5002/Heteroskedasticity.png 'Figure 4.3: Heteroskedasticity 1')

![]({{site.url}}/assets/images/2020/ECON5002/Heteroskedasticity2.png 'Figure 4.4: Heteroskedasticity 2')

**Theorem 2.2 (The sampling variance of the OLS estimators)**  
&emsp;**å®šç†2.2ï¼ˆOLSä¼°è®¡é‡çš„æŠ½æ ·æ–¹å·®ï¼‰**

Under Assumptions SLR.1-SLR.5  
&emsp;åœ¨å‡è®¾SLR.1-SLR.5ä¸‹

$$
  Var(\hat{\beta} _1 | x) 
  = \frac {\sigma ^2}{\sum _{i=1}^n (x _i - \overline{x}) ^2}
  = \frac {\sigma ^2}{\text{SST} _x}
  \tag{4.2} \label{varianceEstimator1}
$$

and  
&emsp;å’Œ

$$
  Var(\hat{\beta} _0 | x) 
  = \frac {\sigma ^2 n ^{-1} \sum _{i=1}^n x _i^2}{\sum _{i=1}^n (x _i - \overline{x}) ^2}
  \tag{4.3} \label{varianceEstimator2}
$$

where \\( x = \lbrace x _1, \cdots, x _2 \rbrace \\) (sample values).  
&emsp;å…¶ä¸­\\( x = \lbrace x _1, \cdots, x _2 \rbrace \\)ï¼ˆç¤ºä¾‹å€¼ï¼‰ã€‚

But \\( \sigma ^2\\) is unknown! And \\(u _i s\\) are not observable!  
&emsp;ä½†æ˜¯\\( \sigma ^2\\)æ˜¯æœªçŸ¥çš„ï¼å’Œ\\(u _i s\\)æ˜¯ä¸å¯è§çš„ï¼

**Theorem 2.3 (The unbiased estimator of \\( \sigma ^2\\))**  
&emsp;**å®šç†2.3ï¼ˆ\\(\sigma^2\\)çš„æ— åä¼°è®¡ï¼‰**

$$
  E(\hat{\sigma} ^2) = \sigma ^2,
  \ \hat{\sigma} ^2 = \frac{\sum _{i=1}^n \hat{u} _i^2}{n-2}.
$$

Why (n - 2)? Recall the OLS first order conditions:  
&emsp;ä¸ºä»€ä¹ˆæ˜¯ï¼ˆn - 2ï¼‰ï¼Ÿå›å¿†æœ€å°äºŒä¹˜æ³•çš„ä¸€é˜¶æ¡ä»¶ï¼š

$$
  \sum _{i=1}^n \hat{u} _i = 0,
  \ \sum _{i=1}^n \hat{u} _i x _i = 0.
$$

- If \\( \hat{\sigma} ^2\\) is plugged in \eqref{varianceEstimator1} and \eqref{varianceEstimator2} we get an estimate of the variance.  
&emsp;å¦‚æœ\\( \hat{\sigma} ^2\\) æ’å…¥äº†å…¬å¼\eqref{varianceEstimator1}å’Œ\eqref{varianceEstimator2}ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†æ–¹å·®çš„ä¼°è®¡ã€‚

- Similarly, the **standard errors (se)** of \\(\hat{\beta} _0 \\) and \\(\hat{\beta} _1 \\).  
&emsp;ç±»ä¼¼åœ°ï¼Œ\\(\hat{\beta} _0 \\)å’Œ\\(\hat{\beta} _1 \\)çš„**æ ‡å‡†é”™è¯¯ï¼ˆseï¼‰**ã€‚

$$
  \begin{array}{m}
      se(\hat{\beta} _1) 
      = \hat{\sigma} 
        \sqrt{\frac{1}{\sum _{i=1}^n (x _i - \overline{x}) ^2}}
      \\\\ se(\hat{\beta} _0) 
      = \hat{\sigma} ^2 
        \sqrt{\frac{n ^{-1} \sum _{i=1}^n x _i^2}{\sum _{i=1}^n (x _i - \overline{x}) ^2}}
  \end{array}
$$

are estimates of the standard deviation of the estimators.  
&emsp;æ˜¯ä¼°è®¡é‡çš„æ ‡å‡†å·®çš„ä¼°è®¡ã€‚

![]({{site.url}}/assets/images/2020/ECON5002/estimatedDensity.png 'Figure 4.5: The estimated density.')

### Takeaway  
&emsp;è¯¾å¤–

- Assumptions SLR.1-SLR.4 are sucient to show that \\(\hat{\beta} _0\\), \\(\hat{\beta} _0\\) are unbiased.  
&emsp;å‡è®¾SLR.1-SLR.4æœ‰åŠ©äºè¯æ˜\\(\hat{\beta} _0\\)ï¼Œ\\(\hat{\beta} _0\\)æ˜¯æ— åçš„ã€‚

- SLR stands for Simple Linear Regression.  
&emsp;SLRä»£è¡¨ç®€å•çº¿æ€§å›å½’ã€‚

- Because \\( E(y _i \| x _i) = \beta _0 + \beta _1 x _i \\) (SLR.4), estimating \\(\hat{\beta} _0\\) and \\(\hat{\beta} _1\\) mean estimating the CEF.  
&emsp;å› ä¸º\\( E(y _i \| x _i) = \beta _0 + \beta _1 x _i \\) (SLR.4)ï¼Œä¼°è®¡\\(\hat{\beta} _0\\)å’Œ\\(\hat{\beta} _1\\)æ„å‘³ç€ä¼°è®¡CEFã€‚

- SLR.5 is added to obtain the variance of the OLS estimators.  
&emsp;åŠ å…¥SLR.5å¾—åˆ°OLSä¼°è®¡é‡çš„æ–¹å·®ã€‚

- Mean and Variance refer to random variables! Here we considered the OLS estimators, not the estimates.  
&emsp;å‡å€¼å’Œæ–¹å·®æŒ‡çš„æ˜¯éšæœºå˜é‡ï¼è¿™é‡Œæˆ‘ä»¬è€ƒè™‘çš„æ˜¯OLSä¼°è®¡ï¼Œè€Œä¸æ˜¯ä¼°è®¡ã€‚

## 5. Counterfactual Outcomes and Causality  
&emsp;äº”. åäº‹å®ç»“æœå’Œå› æœå…³ç³»

### Causal Effects  
&emsp;å› æœå…³ç³»

Let \\(x _i\\) a binary variable (either \\(x _i = 1 \text{ or } x _i = 0\\) ).  
&emsp;è®¾\\(x _i\\)ä¸€ä¸ªäºŒè¿›åˆ¶å˜é‡ï¼ˆè¦ä¹ˆ\\(x _i = 1 \text{ or } x _i = 0\\)ã€‚

- \\(x _i = 1 \\) indicates a treatment.  
&emsp;\\(x _i=1\\)è¡¨ç¤ºæ²»ç–—ã€‚

- \\(x _i = 0 \\) indicates a non-treatment.  
&emsp;\\(x _i=0\\)è¡¨ç¤ºä¸æ²»ç–—ã€‚

We are interested in the causal effect of \\(x _i\\)  on \\(y _i (x _i)\\). The difference between the two **potential** outcomes:  
&emsp;æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯\\(x _i\\)å¯¹\\(y _i (x _i)\\)çš„å› æœå…³ç³»ã€‚ä¸¤ç§**æ½œåœ¨**ç»“æœä¹‹é—´çš„å·®å¼‚ï¼š

$$
  \tau _i = y _i (1) - y _i (0)
$$

is called the **causal** (or **treatment**) effect.  
&emsp;è¢«ç§°ä¸º**å› æœ**ï¼ˆæˆ–**æ²»ç–—**ï¼‰æ•ˆåº”ã€‚

Example (the Perry Preschool Project):  
&emsp;ä¾‹å¦‚ï¼ˆä½©é‡Œå¹¼å„¿å›­é¡¹ç›®ï¼‰ï¼š

- \\( y _i (1) \\) is the wage of the individual \\( i \\) if he/she entered the preschool program (\treatment group").  
&emsp;\\( y _i (1) \\) æ˜¯ä¸ªäººiçš„å·¥èµ„ï¼Œå¦‚æœä»–/å¥¹è¿›å…¥å­¦å‰æ•™è‚²é¡¹ç›®ï¼ˆ\æ²»ç–—ç»„â€œï¼‰ã€‚

- \\( y _i (0) \\) is the wage of the individual \\( i \\) if he/she received no preschool education (\control group").  
&emsp;\\( y _i (0) \\)æ˜¯æœªæ¥å—å­¦å‰æ•™è‚²çš„ä¸ªäººiçš„å·¥èµ„ï¼ˆi\\)ï¼ˆ\å¯¹ç…§ç»„ï¼‰ã€‚

- For the individual \\( i \\) we only observe one the two potential outcomes.  
&emsp;å¯¹äºä¸ªä½“iè€Œè¨€ï¼Œæˆ‘ä»¬åªè§‚å¯Ÿåˆ°ä¸¤ç§æ½œåœ¨ç»“æœä¸­çš„ä¸€ç§ã€‚

For each unit \\( i \\), the observed outcome \\( y _i \\) can be written as:  
&emsp;å¯¹äºæ¯ä¸ªå•ä½\\(i \\)ï¼Œè§‚å¯Ÿåˆ°çš„ç»“æœ\\(y i \\)å¯ä»¥å†™å‡ºæ¥:

$$
  y _i 
  = (1-x _i)y _i (0) + y _i (1) x _i 
  = y _i (0) + [y _i(1) + y _i (0)]x _i 
  \tag{5.1} \label{observedOutcome}
$$

Check that  
&emsp;æ£€æŸ¥

$$
  y _i =
  \begin{cases}
  y _i (1) \text{ if } x _i = 1,
  \\\\
  y _i (0) \text{ if } x _i = 0.
  \end{cases}
$$

Equation \eqref{observedOutcome} can be rewritten as  
&emsp;ç­‰å¼\eqref{observedOutcome}å¯ä»¥è¢«é‡æ–°ä¸º

$$
  y _i = y _i(0) + \tau _i x _i
  \tag{5.2}
$$

We cannot hope to estimated \\( \tau _i \\) for each unit \\( i \\).  
&emsp;æˆ‘ä»¬å¸Œæœ›ä¸ºæ¯ä¸€ä¸ªå•ä½iä¼°è®¡å‡º\\( \tau _i \\)

Our aim will be to estimate the **average treatment (or causal) effect (ATE)**:  
&emsp;æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼°è®¡**å¹³å‡æ²»ç–—ï¼ˆæˆ–å› æœï¼‰æ•ˆåº”ï¼ˆATEï¼‰**ï¼š

$$
  \tau _{ate} = E(\tau _i) = E[y _i (1) - y _i (0)] = \alpha _1 - \alpha _0
$$

where  
&emsp;å…¶ä¸­

$$
  \alpha _1 = E[y _i (1)] \text{, and } \alpha _0 = E[y _i (0)]
$$

Define  
&emsp;å®šä¹‰

$$
  y _i (1) = \alpha _1 + u _i (1) 
  \text{  and  } 
  y _i (0) = \alpha _0 + u _i (0)
$$

Then, equation  
&emsp;é‚£ä¹ˆï¼Œç­‰å¼

$$
  y _i = y _i (0) + \tau _i  x _i
  \tag{5.3}
$$

can be written as  
&emsp;å¯ä»¥è¢«å†™ä½œï¼š

$$
  y _i = \alpha + \tau _{ate} \cdot x _i + u _i
  \tag{5.4}
$$

where \\( \tau _{ate} = \alpha _1 - \alpha _0\\), and  
&emsp;å…¶ä¸­ \\( \tau _{ate} = \alpha _1 - \alpha _0\\)ï¼Œè€Œä¸”

$$
  u _i = u _i(0) + [u _i(1) - u _i(0)] x _i
  \tag{5.5}
$$

If the treatment is randomly assigned  
&emsp;å¦‚æœæ²»ç–—è¢«éšæœºåœ°æŒ‡æ´¾

$$
  x _i \models [u _i(0) , u _i(1)]
  \tag{5.6}
$$

![]({{site.url}}/assets/images/2020/ECON5002/specialCharEq.png 'Equation 5.6: The estimated density.')

(post the clip here because the author has doubts of the character inside this equation)

Hence,  
&emsp;å› æ­¤ï¼Œ

$$
  E[u _i (0) | x _i] = E[u _i (0)], \ E[u _i (1) | x _i] = E[u _i (1)]
  \tag{5.7}
$$

implying  
&emsp;æ„å‘³ç€

$$
  E[u _i | x _i] 
  = E[u _i (0) | x _i] + E[u _i (1) - u _i (0) | x _i] x _i 
  = 0 
  \tag{5.8}
$$

The OLS estimators are unbiased for the \\(\alpha _0 \\) and \\( \tau _{ate} \\)!  
&emsp;OLSä¼°è®¡é‡å¯¹äº \\(\alpha _0 \\)å’Œ\\( \tau _{ate} \\)æ˜¯æ— åçš„ï¼

### Takeaway  
&emsp;è¯¾å¤–

- \\( \tau _{ate} \\) can be estimated by OLS.  
&emsp;\\( \tau _{ate} \\)å¯ä»¥ç”¨OLSä¼°è®¡ã€‚

- Random assignment implies that \\(E[u _i \| x _i]  = 0 \\). We conclude that \\( \hat{\tau} _{ate} \\) is unbiased.  
&emsp;éšæœºèµ‹å€¼æ„å‘³ç€\\(E[u _i \| x _i]  = 0 \\)ã€‚æˆ‘ä»¬çš„ç»“è®ºæ˜¯ \\( \hat{\tau} _{ate} \\)æ˜¯æ— åçš„ã€‚

## Video 6: Multiple regression analysis  
&emsp;è§†é¢‘6ï¼šå¤šå…ƒå›å½’åˆ†æ

### Motivation  
&emsp;åŠ¨æœº

Consider an extension of the regression of *lwage* on *educ*:  
&emsp;è€ƒè™‘*lwage*å¯¹*educ*å›å½’çš„ä¸€ä¸ªæ‰©å±•ï¼š

$$
  lwage = \beta _0 + \beta _1 educ + \beta _2 IQ +u
$$

*IQ* is the Intelligence Quotient score (In population, the *IQ* score has mean 100 and standard deviation 15.)  
&emsp;*IQ*æ˜¯æ™ºå•†å¾—åˆ†ï¼ˆåœ¨äººç¾¤ä¸­ï¼Œ*IQ*å¾—åˆ†çš„å¹³å‡å€¼ä¸º100ï¼Œæ ‡å‡†å·®ä¸º15ã€‚ï¼‰

Primarily interested in \\(\beta _1\\), but \\(\beta _2\\) is of some interest, too. Including *IQ* in the equation, it is taken out of the error term.  
If *IQ* is a good proxy for intelligence, this may lead to a more
persuasive estimate of the causal effect of schooling.  
&emsp;ä¸»è¦å¯¹\\(\beta _1\\)æ„Ÿå…´è¶£ï¼Œä½†\\\(\beta _2\\)ä¹Ÿæœ‰å…´è¶£ã€‚åœ¨æ–¹ç¨‹ä¸­åŠ ä¸ŠIQï¼Œå°±ä»è¯¯å·®é¡¹ä¸­å»æ‰äº†ã€‚
å¦‚æœæ™ºå•†æ˜¯æ™ºåŠ›çš„ä¸€ä¸ªå¾ˆå¥½çš„ä»£è¡¨ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´å¯¹å­¦æ ¡æ•™è‚²å› æœå…³ç³»çš„æ›´å…·è¯´æœåŠ›çš„ä¼°è®¡ã€‚

A model with two independent variables can be written as  
&emsp;æœ‰ä¸¤ä¸ªè‡ªå˜é‡çš„æ¨¡å‹å¯ä»¥å†™æˆ

$$
  y = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + u,
  \tag{6.1}
$$

- \\(\beta _0\\) is the intercept.  
&emsp;\\(\beta _0\\)æ˜¯æˆªè·ã€‚

- \\(\beta _1\\) measures the change in y with respect to \\(x _1 \\), holding other factors fixed.  
&emsp;\\(\beta _1\\)æµ‹é‡yç›¸å¯¹äº\\(x _1\\)çš„å˜åŒ–ï¼Œä¿æŒå…¶ä»–å› ç´ ä¸å˜ã€‚

- \\(\beta _2\\) measures the change in y with respect to \\(x _2 \\), holding other factors fixed.  
&emsp;\\(\beta _2\\)æµ‹é‡yç›¸å¯¹äº\\(x _2\\)çš„å˜åŒ–ï¼Œä¿æŒå…¶ä»–å› ç´ ä¸å˜ã€‚

- *u* is the unobserved component (error).  
&emsp;*u*æ˜¯æœªè§‚æµ‹åˆ°çš„åˆ†é‡ï¼ˆè¯¯å·®ï¼‰ã€‚

In the model with two explanatory variables, the key assumption about how u is related to \\(x _1 \\) and \\(x _2 \\) is  
&emsp;åœ¨æœ‰ä¸¤ä¸ªè§£é‡Šå˜é‡çš„æ¨¡å‹ä¸­ï¼Œå…³äºuå¦‚ä½•ä¸\\(x _1 \\)å’Œ\\(x _2 \\)ç›¸å…³çš„å…³é”®å‡è®¾æ˜¯

$$
  E(u  | x _1, x _2)  = 0 .
  \tag{6.2}
$$

Then \\( E (y \| x _1, x _2 ) = \beta _0 + \beta _1 x _1 + \beta _2 x _2\\) and  
&emsp;é‚£ä¹ˆ\\( E (y \| x _1, x _2 ) = \beta _0 + \beta _1 x _1 + \beta _2 x _2\\)å’Œ

$$
  y = E (y | x _1, x _2 ) + u
$$

In the wage equation, the assumption is \\( E(u \| educ, IQ) = 0\\).  
&emsp;åœ¨å·¥èµ„å…¬å¼ä¸­ï¼Œå‡è®¾ä¸º\\( E( u \| educ, IQ) = 0\\)ã€‚

Now *u* no longer contains intelligence (we hope), and so this assumption has a better chance of being true.  
&emsp;ç°åœ¨ï¼Œ*u*ä¸å†åŒ…å«æ™ºåŠ›ï¼ˆæˆ‘ä»¬å¸Œæœ›å¦‚æ­¤ï¼‰ï¼Œå› æ­¤è¿™ä¸ªå‡è®¾æ›´æœ‰å¯èƒ½æˆä¸ºäº‹å®ã€‚

In the simple regression model, we had to assume *IQ* and *educ* were unrelated to justify \\( E(u \| educ) = 0\\)  (*IQ* was in *u*).  
&emsp;åœ¨ç®€å•å›å½’æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»å‡è®¾*IQ*å’Œ*educ*ä¸justify\\(Eï¼ˆu \| educï¼‰=0\\)ï¼ˆ*IQ*åœ¨*u*ä¸­ï¼‰ã€‚

Other factors, such as workforce experience and @motivation," are part of u. Motivation is very dicult to measure. Measuring experience is easier:  
&emsp;å…¶ä»–å› ç´ ï¼Œå¦‚å·¥ä½œç»éªŒå’Œâ€œæ¿€åŠ±â€ä¹Ÿæ˜¯ç¾å›½çš„ä¸€éƒ¨åˆ†ã€‚æ¿€åŠ±å¾ˆéš¾è¡¡é‡ã€‚æµ‹é‡ä½“éªŒæ›´å®¹æ˜“ï¼š

$$
  lwage = \beta _0 + \beta _1 educ + \beta _2 IQ + \beta _3 exper + u
$$

### The Model with k Explanatory Variables  
&emsp;kè§£é‡Šå˜é‡æ¨¡å‹

The **multiple linear regression model** can be written in the population as  
&emsp;**å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹**å¯ä»¥åœ¨æ€»ä½“ä¸­å†™æˆ

$$
   y = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \cdots +\beta _k x _k + u
$$

\\( \beta _0\\) is the **intercept**, \\( \beta _1\\) is the parameter associated with \\( x _1 \text{, } \beta _2\\) is the parameter associated with  \\( x _2\\), and so on.  
&emsp;\\( \beta _0\\)æ˜¯**æˆªè·**ï¼Œ\\( \beta _1\\)æ˜¯ä¸\\( x _1 \text{, } \beta _2\\)ç›¸å…³çš„å‚æ•°ï¼Œä¾æ­¤ç±»æ¨ã€‚

Contains *k* + 1 (unknown) population parameters. We call \\( \beta _1, \cdots, \beta _k \\) the **slope parameters**.  
&emsp;åŒ…å«*k*+1ï¼ˆæœªçŸ¥ï¼‰æ€»ä½“å‚æ•°ã€‚æˆ‘ä»¬ç§°\\(\beta _1ã€\cdotsã€\beta _k\\)ä¸º**æ–œç‡å‚æ•°**ã€‚

Multiple regressions allows more flexible functional forms.  
&emsp;å¤šå…ƒå›å½’å…è®¸æ›´çµæ´»çš„å‡½æ•°å½¢å¼ã€‚

$$
  lwage = \beta _0 + \beta _1 educ + \beta _2 IQ + \beta _3 exper 
    + \beta _4 exper^2+ u 
$$

&emsp;so that exper is allowed to have a quadratic effect on *lwage*.  
è¿™æ ·experå°±å¯ä»¥å¯¹*lwage*äº§ç”ŸäºŒæ¬¡æ•ˆåº”ã€‚

Let \\( x _1 = educ, x _2 = IQ, x _3 = exper, and x _4 = exper ^2 \\).  
&emsp;å‡è®¾ä¸Šè¿°æƒ…å†µã€‚

Note that \\(x _4\\) is a a nonlinear function of \\(x _3\\), but the model is still linear in the parameters!  
&emsp;æ³¨æ„ï¼Œ\\(x _4\\)æ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œ\\(x _3\\)ï¼Œä½†æ˜¯æ¨¡å‹çš„å‚æ•°ä»ç„¶æ˜¯çº¿æ€§çš„ï¼

We already know that \\(100 \cdot \beta _1\\) is the ceteris paribus percentage change in *wage* when *educ* increases by one year.  
&emsp;æˆ‘ä»¬å·²ç»çŸ¥é“ï¼Œ\\(100 \cdot \beta _1\\)æ˜¯å½“æ•™è‚²è´¹å¢åŠ ä¸€å¹´æ—¶ï¼Œå·¥èµ„çš„å…¶ä»–åŒç­‰ç™¾åˆ†æ¯”å˜åŒ–ã€‚

\\(100 \cdot \beta _2\\) has a similar interpretation (for \\(\Delta IQ = 1\\)).  
&emsp;\\(100 \cdot \beta _2\\)æœ‰ç±»ä¼¼çš„è§£é‡Šï¼ˆå¯¹äº\\(\Delta IQ = 1\\)ï¼‰ã€‚

\\( \beta _3\\)  and \\( \beta _4\\) are harder to interpret. Using calculus,  
&emsp;\\( \beta _3\\)å’Œ\\( \beta _4\\)æ›´éš¾ç†è§£ã€‚ç”¨å¾®ç§¯åˆ†ï¼Œ

$$
  \frac{\partial lwage} {\partial exper} 
  = \beta _3 + 2 \beta _4 exper
$$

&emsp; Multiply by 100 to get the percentage effect.  
&emsp;&emsp;ä¹˜ä»¥100å¾—åˆ°ç™¾åˆ†æ¯”æ•ˆæœã€‚

### Takeaway  
&emsp;è¯¾å¤–

The multiple regression model allows us:  
&emsp;å¤šå…ƒå›å½’æ¨¡å‹å…è®¸æˆ‘ä»¬ï¼š

- to control for many other factors that simultaneously affect the dependent variables (ceteris paribus interpretation),  
&emsp;æ§åˆ¶åŒæ—¶å½±å“å› å˜é‡çš„è®¸å¤šå…¶ä»–å› ç´ ï¼ˆå…¶ä»–å› ç´ åŒç­‰è§£é‡Šï¼‰ï¼Œ
 
- to model non-linear relationships (linearity is in the parameters!).  
&emsp;å»ºç«‹éçº¿æ€§å…³ç³»æ¨¡å‹ï¼ˆçº¿æ€§åœ¨å‚æ•°ä¸­ï¼ï¼‰ã€‚

## Video 7: Mechanics and Interpretation of the Ordinary Least Squares  
&emsp;è§†é¢‘7ï¼šæ™®é€šæœ€å°äºŒä¹˜æ³•çš„åŠ›å­¦å’Œè§£é‡Š

### Interpreting the OLS Regression Line  
&emsp;è§£é‡ŠOLSå›å½’çº¿

Consider the case *k* = 2:  
&emsp;è€ƒè™‘æƒ…å†µ*k*=2ï¼š

$$
  \hat{y} = \hat{\beta} _0 + \hat{\beta} _1 x _1 + \hat{\beta} _2 x _2
  \tag{7.1}
$$

The intercept\\( \hat{\beta} _0 \\)is the predicted value of *y* when \\( x _1 = x _2 = 0 \\)  
&emsp;æˆªè·\\( \hat{\beta} _0 \\)æ˜¯å½“\\( x _1 = x _2 = 0 \\)æ—¶*y*çš„é¢„æµ‹å€¼

The estimates \\( \hat{\beta} _1, \ \hat{\beta} _2\\) have **partial effects**, or **ceteris paribus** interpretations.  
&emsp;ä¼°è®¡æ•°\\( \hat{\beta} _1, \ \hat{\beta} _2 \\)å…·æœ‰**éƒ¨åˆ†æ•ˆåº”**ï¼Œæˆ–**å…¶ä»–åŒç­‰**è§£é‡Šã€‚


If we "hold \\(x _2 \\) fixed"  
&emsp;å¦‚æœæˆ‘ä»¬â€œä¿æŒ\\(x _2 \\)å›ºå®š"


$$
  \Delta \hat{y}
  = \hat{\beta} _1 \Delta x _1 
  \text{ if } \Delta x _2 = 0
$$

\\( \beta _1\\) measures the predicted change in *y* due a one-unit increase in \\(x _1 \\), holding \\(x _2 \\) fixed.  
&emsp;\\( \beta _1\\) è¡¡é‡ç”±äº\\(x _1 \\)å¢åŠ ä¸€ä¸ªå•ä½ï¼ŒæŒæœ‰\\(x _2 \\)å›ºå®šä¸å˜è€Œå¯¼è‡´çš„*y*çš„é¢„æµ‹å˜åŒ–ã€‚

Similarity,  
&emsp;ç›¸ä¼¼çš„ï¼Œ

$$
  \hat{\beta} _2 = \frac{\Delta \hat{y}}{\Delta x _2} 
  \text{ if } \Delta x _1 = 0
$$

**Example**  
&emsp;ä¾‹å­

Dataset1 WAGE2.DTA(Wooldridge, online resources):

$$
  \begin{array}{m}
    \widehat{lwage} = 1.142 + .099 educ
    \\\\n = 759
  \end{array}
$$

and  
&emsp;å’Œ

$$
  \begin{array}{m}
    \widehat{lwage} = .728 + .073 educ + .0076IQ
    \\\\n = 759
  \end{array}
$$

The predicted return to a year of education falls from about 9:9% to about 7:3% when we control for differences in *IQ*.  
&emsp;å¦‚æœæˆ‘ä»¬æ§åˆ¶æ™ºå•†çš„å·®å¼‚ï¼Œé¢„è®¡ä¸€å¹´çš„æ•™è‚²å›æŠ¥ç‡ä»9:9%ä¸‹é™åˆ°7:3%ã€‚

- The simple regression does not allow us to compare people with the same *IQ* score.  
&emsp;ç®€å•çš„å›å½’ä¸å…è®¸æˆ‘ä»¬æ¯”è¾ƒé‚£äº›æ™ºå•†ç›¸åŒçš„äººã€‚

- The larger estimated return from simple regression is because we are attributing part of the *IQ* effect to education.  
&emsp;ç®€å•å›å½’å¾—åˆ°çš„æ›´å¤§çš„ä¼°è®¡å›æŠ¥æ˜¯å› ä¸ºæˆ‘ä»¬æŠŠéƒ¨åˆ†æ™ºå•†æ•ˆåº”å½’å› äºæ•™è‚²ã€‚

- Not surprisingly, \\( Corr(educ, IQ) = :573 \\).  
&emsp;æ¯«ä¸å¥‡æ€ªï¼Œ\\( Corr(educ, IQ) = :573 \\)ã€‚

### Comparing Simple and Multiple Regression Estimates  
&emsp;æ¯”è¾ƒç®€å•å’Œå¤šå…ƒå›å½’ä¼°è®¡

Consider the simple and multiple OLS regression functions:  
&emsp;è€ƒè™‘ç®€å•å’Œå¤šé‡OLSå›å½’å‡½æ•°ï¼š

$$
  \widetilde{y} _i = \widetilde{\beta} _0 + \widetilde{\beta} _1 x _{1i}
  \tag{7.2}
$$

$$
  \hat{y} _i = \hat{\beta} _0 + \hat{\beta} _1 x _{1i} + \hat{\beta} _2 x _{2i}
  \tag{7.3} 
$$

using the same *n* observations.  
&emsp;ä½¿ç”¨ç›¸åŒçš„*n*è§‚æµ‹å€¼ã€‚

Question: Is there a relationship between \\( \beta _1\\) (which does not control for \\( x _2\\) ) and \\( \beta _1\\) (which does)?  
&emsp;é—®é¢˜ï¼š\\(\beta _1\\)ï¼ˆä¸æ§åˆ¶\\(x _2\\)ï¼‰å’Œ\\(\beta _1\\)ï¼ˆæ§åˆ¶x _2\\)ä¹‹é—´æœ‰å…³ç³»å—ï¼Ÿ

Yes, but we need to define another simple regression.  
&emsp;æ˜¯çš„ï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦å®šä¹‰å¦ä¸€ä¸ªç®€å•çš„å›å½’ã€‚

Regressing \\( x _2\\) on \\( x _1\\) and a constant we obtain  
&emsp;åœ¨\\(x _1\\)ä¸Šå›å½’\\(x _2\\)ï¼Œå¾—åˆ°ä¸€ä¸ªå¸¸æ•°

$$
  x _{2i} = \widetilde{\delta} _0 + \widetilde{\delta} _1 x _{1i} + \widetilde{r} _i
  \tag{7.4}
$$

Then,  
&emsp;é‚£ä¹ˆï¼Œ

$$
  \begin{align}
    \hat{y} _i & = \hat{\beta} _0 + \hat{\beta} _1 x _{1i} + \hat{\beta} _2 x _{2i} \tag{7.5}
    \\\\ & = \left( \hat{\beta} _0 + \hat{\beta} _1 \widetilde{\delta} _0 \right)
    + \left( \hat{\beta} _1 + \hat{\beta} _2 \widetilde{\delta} _1 \right) x _{1i} + \hat{\beta} _2 \widetilde{r} _i \tag{7.6}
  \end{align}
$$

&emsp;where \\( \sum _{i=1}^n x _1 \widetilde{r} _i =0 \\) by construction.  
&emsp;&emsp;å…¶ä¸­\\( \sum _{i=1}^n x _1 \widetilde{r} _i =0 \\)ç”±æ„é€ ã€‚

Regressing y on \\( x _1\\) and a constant only or on \\([x _1, \widetilde{r}]\\) and a constant gives the same estimate of the coecient of \\( x _1\\).  
&emsp;å°†yåœ¨\\( x _1\\)ä¸Šå’Œä¸€ä¸ªå¸¸æ•°æˆ–ä»…åœ¨\\([x _1, \widetilde{r}]\\)ä¸Šå’Œä¸€ä¸ªå¸¸æ•°ä¸Šå›å½’å¾—åˆ°\\( x _1\\)ç³»æ•°çš„ç›¸åŒä¼°è®¡ã€‚

For any sample  
&emsp;å¯¹äºä»»ä½•æ ·æœ¬

$$
  \widetilde{\beta} _1 
  = \hat{\beta} _1 + \hat{\beta} _2 \widetilde{\delta} _1
  \tag{7.7}
$$

If the partial effect of \\( x _2\\) on *y* is positive, so \\( \widetilde{\beta} _2 > 0\\), and \\( x _1\\) and \\( x _2\\) are positive correlated in the sample, so \\( \widetilde{\delta} > 0 \\), then  
&emsp;å¦‚æœ\\( x _2\\)å¯¹*y*çš„éƒ¨åˆ†æ•ˆåº”æ˜¯æ­£çš„ï¼Œé‚£ä¹ˆ\\( \widetilde{\beta} _2 > 0\\)å’Œ\\( x _1\\)å’Œ\\( x _2\\)åœ¨æ ·æœ¬ä¸­æ˜¯æ­£ç›¸å…³çš„ï¼Œæ‰€ä»¥\\( \widetilde{\delta} > 0 \\)ï¼Œé‚£ä¹ˆ

$$
  \widetilde{\beta} _1 > \hat{\beta} _1
$$

**Example**  
&emsp;ä¾‹å­

![]({{site.url}}/assets/images/2020/ECON5002/example.png "lwage example")

### Takeaway  
&emsp;è¯¾å¤–

- In the multiple regression model, the slopes measure the partial effects of the corresponding regressor on the dependent variable, holding all other regressors fixed.  
&emsp;åœ¨å¤šå…ƒå›å½’æ¨¡å‹ä¸­ï¼Œæ–œç‡æµ‹é‡äº†ç›¸åº”å›å½’å› å­å¯¹å› å˜é‡çš„éƒ¨åˆ†å½±å“ï¼Œä¿æŒæ‰€æœ‰å…¶ä»–å›å½’å› å­ä¸å˜ã€‚

- Comparing the single and the bivariate model will be helpful to understand the omitted variable bias.  
&emsp;æ¯”è¾ƒå•å˜é‡æ¨¡å‹å’ŒåŒå˜é‡æ¨¡å‹æœ‰åŠ©äºç†è§£å¿½ç•¥çš„å˜é‡åå·®ã€‚

## Video 8: Expected Value and Variance of the OLS estimators  
&emsp;è§†é¢‘8:OLSä¼°è®¡é‡çš„æœŸæœ›å€¼å’Œæ–¹å·®

### The Expected Value of the OLS Estimators  
&emsp;OLSä¼°è®¡é‡çš„æœŸæœ›å€¼

As for the simple regression, we can specify a set of assumptions under which OLS is unbiased.  
&emsp;å¯¹äºç®€å•å›å½’ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šä¸€ç»„å‡è®¾ï¼Œåœ¨è¿™äº›å‡è®¾ä¸‹OLSæ˜¯æ— åçš„ã€‚

We will also explicitly consider the bias caused by omitting a variable that appears in the population model.  
&emsp;æˆ‘ä»¬è¿˜å°†æ˜ç¡®è€ƒè™‘ç”±äºå¿½ç•¥äººå£æ¨¡å‹ä¸­å‡ºç°çš„å˜é‡è€Œå¯¼è‡´çš„åå·®ã€‚

The assumptions are labelled "MLR" (multiple linear regression).  
&emsp;è¿™äº›å‡è®¾è¢«æ ‡è®°ä¸ºâ€œå¤šå…ƒçº¿æ€§å›å½’â€ã€‚

**Assumption MLR.2 (Random Sampling)**  
&emsp;å‡è®¾MLR.2ï¼ˆéšæœºæŠ½æ ·ï¼‰

We have a random sample of size n from the population, \\( \lbrace [x _{1i}, x _{2i}, \cdots, x _{ki}, y _i]: i = 1, \cdots,n \rbrace \\)  
&emsp;æˆ‘ä»¬ä»äººç¾¤ä¸­éšæœºæŠ½å–äº†ä¸€ä¸ªå¤§å°ä¸ºnçš„æ ·æœ¬ï¼Œ\\( \lbrace [x _{1i}, x _{2i}, \cdots, x _{ki}, y _i]: i = 1, \cdots,n \rbrace \\)

**Assumption MLR.1 (Linear in Parameters)**  
&emsp;å‡è®¾MLR.1ï¼ˆå‚æ•°çº¿æ€§ï¼‰

For every unit "i", the model in the population can be written as  
&emsp;å¯¹äºæ¯ä¸ªå•ä½â€œiâ€ï¼Œäººå£ä¸­çš„æ¨¡å‹å¯ä»¥å†™æˆ

$$
  y _i = \beta _0 + \beta _1 x _{1i} + \beta _2 x _{2i} + \cdots
    + \beta _k x _{ki} + u _i
$$

where the \\(\beta _k\\) are the population parameters and ui is the unobserved error.  
&emsp;å…¶ä¸­ï¼Œ\\(\beta _k\\)æ˜¯å¡«å……å‚æ•°ï¼Œuiæ˜¯æœªè§‚å¯Ÿåˆ°çš„é”™è¯¯ã€‚

**Assumption MLR.3 (No Perfect Collinearity)**  
&emsp;**å‡è®¾MLR.3ï¼ˆæ— å®Œå…¨å…±çº¿ï¼‰**

In the sample (and in the population), none of the explanatory variables is constant, and there are no exact linear relationships among them.  
&emsp;åœ¨æ ·æœ¬ï¼ˆå’Œæ€»ä½“ï¼‰ä¸­ï¼Œæ²¡æœ‰ä¸€ä¸ªè§£é‡Šå˜é‡æ˜¯æ’å®šçš„ï¼Œå®ƒä»¬ä¹‹é—´ä¹Ÿæ²¡æœ‰ç²¾ç¡®çš„çº¿æ€§å…³ç³»ã€‚

We must rule out the case that one of the explanatory variables is an exact linear function of the others.  
&emsp;æˆ‘ä»¬å¿…é¡»æ’é™¤å…¶ä¸­ä¸€ä¸ªè§£é‡Šå˜é‡æ˜¯å…¶ä»–å˜é‡çš„ç²¾ç¡®çº¿æ€§å‡½æ•°çš„æƒ…å†µã€‚

Under perfect collinearity, there are no unique OLS estimators.  
&emsp;åœ¨å®Œå…¨å…±çº¿æ€§ä¸‹ï¼Œä¸å­˜åœ¨å”¯ä¸€çš„OLSä¼°è®¡ã€‚

Usually perfect collinearity arises from a bad model specification.  
&emsp;é€šå¸¸ï¼Œå®Œç¾çš„å…±çº¿æ€§æ¥è‡ªäºç³Ÿç³•çš„æ¨¡å‹è§„æ ¼ã€‚

Perfect collinearity could also arise because of small sample size or bad luck in drawing. e.g. *educ/IQ *= constant.  
&emsp;ç”±äºæ ·æœ¬é‡å°æˆ–ç»˜å›¾è¿æ°”ä¸å¥½ï¼Œä¹Ÿå¯èƒ½å‡ºç°å®Œå…¨å…±çº¿ã€‚eã€ g.*educ/IQ*=å¸¸æ•°ã€‚

Assumption MLR.3 can only hold if \\( n > k + 1 \\).  
&emsp;å‡è®¾MLR.3åªèƒ½åœ¨\\(n>k+1\\)æ—¶æˆç«‹ã€‚

**Assumption MLR.4 (Zero Conditional Mean)**  
&emsp;**å‡è®¾MLR.4ï¼ˆé›¶æ¡ä»¶å¹³å‡å€¼ï¼‰**

$$
  E(u _i | x _{1i}, x _{2i}, \cdots, x _{ki}) = 0
$$

**EXAMPLE: Effects of Class Size on Student Performance**  
&emsp;**ä¾‹ï¼šç­çº§è§„æ¨¡å¯¹å­¦ç”Ÿæˆç»©çš„å½±å“**

Suppose, for a standardized test score,  
&emsp;å‡è®¾ï¼Œå¯¹äºæ ‡å‡†åŒ–è€ƒè¯•åˆ†æ•°ï¼Œ

$$
  score = \beta _0 + \beta _1 classize + \beta _2 income + u
$$

Even at the same income level, families differ in their interest and concern about their children's education.  
&emsp;å³ä½¿åœ¨ç›¸åŒçš„æ”¶å…¥æ°´å¹³ä¸‹ï¼Œå®¶åº­å¯¹å­å¥³æ•™è‚²çš„å…´è¶£å’Œå…³æ³¨ç¨‹åº¦ä¹Ÿä¸å°½ç›¸åŒã€‚

Family support and student motivation are in *u*.  
&emsp;å®¶åº­æ”¯æŒå’Œå­¦ç”ŸåŠ¨åŠ›åœ¨*u*ä¸­ã€‚

If the omitted factors are correlated with *classize* and *income*ï¼Œ**Assumption MLR.4 fails**.  
&emsp;å¦‚æœçœç•¥çš„å› ç´ ä¸*åˆ†ç±»*å’Œ*æ”¶å…¥*ç›¸å…³ï¼Œ**å‡è®¾MLR.4å¤±è´¥**ã€‚ 

**Theorem 2.1. Unbiasedness of OLS**  
&emsp;**å®šç†2.1ã€‚OLSçš„æ— åæ€§**

Under Assumption MLR.1-MLR.4  
&emsp;å‡è®¾MLR.1-MLR.4

$$
  E \left( \hat{\beta} _j \right) = \beta _j \text{, for } j = 0, 1, \cdots, k
$$

### Inclusion of Irrelevant Variables  
&emsp;åŒ…å«æ— å…³å˜é‡

Suppose, then, that we specify the model  
&emsp;é‚£ä¹ˆï¼Œå‡è®¾æˆ‘ä»¬æŒ‡å®šäº†æ¨¡å‹

$$
  lwage = \beta _0 + \beta _1 educ + \beta _2 exper 
    + \beta _3 mothexper + u 
$$

where MLR.1 through MLR.4 hold.  
&emsp;1å·åˆ°4å·ä¼ é€å™¨æ‰€åœ¨ä½ç½®ã€‚

Suppose that \\( \beta _3 = 0 \\), but we do not know that. We estimate:  
&emsp;å‡è®¾\\( \beta _3 = 0 \\)ï¼Œä½†æˆ‘ä»¬ä¸çŸ¥é“ã€‚æˆ‘ä»¬ä¼°è®¡ï¼š

$$
  \widehat{lwage} = \hat{\beta} _0 + \hat{\beta} _1 educ + \hat{\beta} _2 exper + \hat{\beta} _3 motheDUC + u 
$$

We know from the unbiasedness result that  
&emsp;æˆ‘ä»¬ä»æ— åç»“æœä¸­å¯ä»¥å¾—çŸ¥

$$
  E \left ( \hat{\beta} _j \right) = \beta _j 
  \text{, } j = 0,1,2 \text{, and }
  E \left ( \hat{\beta} _3 \right) = 0
$$

### Omitted Variable Bias  
&emsp;çœç•¥å˜é‡åå·®

Leaving a variable out when it should be including in multiple regression could lead to the **omitted variable bias**.  
&emsp;å½“ä¸€ä¸ªå˜é‡åº”è¯¥åŒ…å«åœ¨å¤šå…ƒå›å½’ä¸­æ—¶ï¼Œå°†å…¶å¿½ç•¥å¯èƒ½ä¼šå¯¼è‡´**å¿½ç•¥å˜é‡åå·®**ã€‚

Assume that the correct model has two explanatory variables:  
&emsp;å‡è®¾æ­£ç¡®çš„æ¨¡å‹æœ‰ä¸¤ä¸ªè§£é‡Šå˜é‡ï¼š

$$
  y = \beta _0 + \beta _1 x _1 + \beta _2 x _2  + u
$$

But suppose we leave out \\( x _2\\) and use simple regression of *y* on \\( x _1\\).  
&emsp;ä½†æ˜¯å‡è®¾æˆ‘ä»¬çœç•¥äº†\\(x _2 \\)ï¼Œä½¿ç”¨ç®€å•çš„å›å½’*y*åœ¨\\(x _1 \\)ä¸Šã€‚

We should have estimated  
&emsp;æˆ‘ä»¬åº”è¯¥ä¼°è®¡

$$
 \hat{y} = \hat{\beta} _0 + \hat{\beta} _1 x _1 + \hat{\beta} _2 x _2
$$

but we estimate  
&emsp;ä½†æ˜¯æˆ‘ä»¬ä¼°è®¡

$$
  \widetilde{y} = \widetilde{\beta} _0 + \widetilde{\beta} _1 x _1
$$

Recall that  
&emsp;å›é¡¾

$$
  \widetilde{\beta} _1 = \hat{\beta} _1 + \hat{\beta} _2 \widetilde{\delta} _1
$$

where  
&emsp;å…¶ä¸­

$$
  \hat{x} _2 = \widetilde{\delta} _0 + \widetilde{\delta} _1 x _1
$$

Conditional on the sample values of \\( x _1\\) and \\( x _2\\).  
&emsp;ä»¥æ ·æœ¬å€¼\\(x _1\\)å’Œ\\( x _2\\)ä¸ºæ¡ä»¶ã€‚

$$
  E \left( \widetilde{\beta} _1 | x \right ) 
  = E \left( \hat{\beta} _1 | x \right) 
    + \widetilde{\delta} _1 E \left( \hat{\beta} _2 | x \right) 
  = \beta _1 + \beta _2 \widetilde{   \delta} _1
$$

Bias \\( \left( \widetilde{\beta} _1 \| x \right) = E \left( \widetilde{\beta} _1 \| x \right) - \beta _1 = \beta _2 \widetilde{\delta} _1 \\)  
&emsp;ä¸Šè¿°åå·®

The bias can be computed for any \\( \lbrace (x _{1i}, x _{2i}), i = 1, \cdots,n \rbrace \\), but if \\(i n \\rightarrow \infty \\) the bias will approach  
&emsp;å¯ä»¥ä¸ºä»»ä½•\\(\lbraceï¼ˆx{1i}ï¼Œx{2i}ï¼‰ï¼Œi=1ï¼Œ\cdotsï¼Œn\rbrace\\)è®¡ç®—åå·®ï¼Œä½†æ˜¯å¦‚æœ\\(i n\\rightarrow\infty\\)ï¼Œåå·®å°†æ¥è¿‘

$$
  \frac{Cov(x _{1i}, X _{2i})} {Var (x _{1i})} \beta _2
$$

The simple regression estimator is unbiased (for the given outcomes \\( \lbrace (x _{1i}, x _{2i}), i = 1, \cdots,n \rbrace \\) in two cases:  
&emsp;ç®€å•å›å½’ä¼°è®¡åœ¨ä¸¤ç§æƒ…å†µä¸‹æ˜¯æ— åçš„ï¼ˆå¯¹äºç»™å®šçš„ç»“æœ\\(\lbraceï¼ˆx{1i}ï¼Œx{2i}ï¼‰ï¼Œi=1ï¼Œ\cdotsï¼Œn\rbrace\\)ï¼š

1. \\( \beta _2 = 0 \\) ( \\(x_2\\) does not appear in the population model),  
&emsp;\\(\beta _2=0\\)ï¼ˆ\\(x _2\\)æœªå‡ºç°åœ¨æ€»ä½“æ¨¡å‹ä¸­ï¼‰ï¼Œ


2. \\( \widehat{Corr} (x _{1i}, x _{2i}) = 0 \\) (in the sample), entailing \\( \widetilde{\delta} _1 = 0 \\)  
&emsp;\\(\widehat{Corr}ï¼ˆx{1i}ï¼Œx{2i}ï¼‰=0\\)ï¼ˆåœ¨ç¤ºä¾‹ä¸­ï¼‰ï¼ŒåŒ…å«\\(\widetilde{\delta}u1=0\\)


### The Variance of the OLS Estimators  
&emsp;OLSä¼°è®¡é‡çš„æ–¹å·®

**Assumption MLR.5 (Homoskedasticity)**  
&emsp;**å‡è®¾MLR.5ï¼ˆåŒæ„æ€§ï¼‰**

The conditional variance of the error, ui, does not change with any of \\( x _{1i}, x _{2i}, \cdots, x _{ki} \\) :  
&emsp;é”™è¯¯çš„æ¡ä»¶æ–¹å·®uiä¸éš\\(x{1i}ï¼Œx{2i}ï¼Œ\cdotsï¼Œx{ki}\\)ä¸­çš„ä»»ä½•ä¸€ä¸ªè€Œæ”¹å˜ï¼š

$$
  Var( u _i | x _{1i}, x _{2i}, \cdots, x _{ki}) = Var(u _i) = \sigma ^2
$$

Assumptions MLR.1 through MLR.5 are called the **Gauss Markov assumptions**.  
&emsp;å‡è®¾MLR.1åˆ°MLR.5è¢«ç§°ä¸º**é«˜æ–¯-é©¬å°”å¯å¤«å‡è®¾**ã€‚

**Theorem 3.2 (Sampling Variances of OLS Slope Estimators)**  
&emsp;**å®šç†3.2ï¼ˆOLSæ–œç‡ä¼°è®¡é‡çš„æŠ½æ ·æ–¹å·®ï¼‰**

Under Assumptions MLR.1 to MLR.5, and condition on the values of the explanatory variables in the sample,  
&emsp;åœ¨å‡è®¾MLR.1åˆ°MLR.5çš„æƒ…å†µä¸‹ï¼Œä»¥æ ·æœ¬ä¸­è§£é‡Šå˜é‡çš„å€¼ä¸ºæ¡ä»¶ï¼Œ

$$
  Var \left ( \hat{\beta} _ j | x \right)
  = \frac{\sigma ^2} {SST _j (1-R _j^2)}
  \text{, } j = 1, 2, \cdots, k.
$$

where  
&emsp;å…¶ä¸­

- \\( SST _j = \sum _{i=1}^n (x _{ji} - \overline{x} _j) ^2\\)

- \\( R _j^2\\) is the \\(R ^2\\) of the regression

$$
  x _{ji} \text{ on } x _{1i}, x _{2i}, \cdots, x _{j-1,i}, x _{j+1,i},\cdots, x _{k,i}
$$

- Adding explanatory variables reduces \\( \sigma ^2 \\).  
&emsp;æ·»åŠ è§£é‡Šæ€§å˜é‡ä¼šå‡å°‘\\(\sigma^2\\)ã€‚

- It is easier to estimate how \\( x _j \\) affects *y* if we see more variation in \\( x _j \\) , and/or *n* is large.  
&emsp;å¦‚æœæˆ‘ä»¬çœ‹åˆ°\\(x _j\\)ä¸­æœ‰æ›´å¤šçš„å˜åŒ–ï¼Œå’Œ/æˆ–*n*å¾ˆå¤§ï¼Œåˆ™æ›´å®¹æ˜“ä¼°è®¡\\(x _j\\)å¦‚ä½•å½±å“*y*ã€‚

- If \\( x _j \\) is unrelated to all other independent variables, it is easier to estimate its ceteris paribus effect on *y*.  
&emsp;å¦‚æœ\\(x _j \\)ä¸æ‰€æœ‰å…¶ä»–è‡ªå˜é‡æ— å…³ï¼Œåˆ™æ›´å®¹æ˜“ä¼°è®¡å®ƒå¯¹*y*çš„å…¶ä»–åŒç­‰å½±å“ã€‚

- As \\( R _j^2 \\rightarrow 1\text{, } Var( \hat{\beta} _j) \\rightarrow 1 \\) (the estimate of \\(\beta _j \\) is not precise).  
&emsp;As\\(R _j^2\\rightarrow 1\text{ï¼Œ}Varï¼ˆ\hat{\beta} _jï¼‰\\rightarrow 1\\)ï¼ˆä¼°è®¡çš„\\(\beta _j\\)ä¸ç²¾ç¡®ï¼‰ã€‚

### Variances in Misspecified Models  
&emsp;é”™è¯¯æŒ‡å®šæ¨¡å‹ä¸­çš„æ–¹å·®

As for bias calculations, we can study the variances of the OLS estimators in misspecified models.  
&emsp;è‡³äºåå·®è®¡ç®—ï¼Œæˆ‘ä»¬å¯ä»¥ç ”ç©¶é”™è¯¯æŒ‡å®šæ¨¡å‹ä¸­OLSä¼°è®¡é‡çš„æ–¹å·®ã€‚

Assume that the model  
&emsp;å‡è®¾æ¨¡å‹

$$
  y = \beta _0 +\beta _1 x _1 + \beta _2 x _2 +u
$$

satisfies the Gauss-Markov assumptions.  
&emsp;æ»¡è¶³Gauss-Markovå‡è®¾ã€‚

We run the misspecified and the correctly specified regressions,  
&emsp;æˆ‘ä»¬è¿è¡Œé”™è¯¯æŒ‡å®šå’Œæ­£ç¡®æŒ‡å®šçš„å›å½’ï¼Œ

$$
  \begin{array}{m}
  \widetilde{y} = \widetilde{\beta} _0 + \widetilde{\beta} _1 x _1
  \\\\ \hat{y} = \hat{\beta} _0 + \hat{\beta} _1 x _1 + \hat{\beta} _2 x _2
  \end{array}  
$$

We know from the previous analysis that  
&emsp;æ ¹æ®å‰é¢çš„åˆ†ææˆ‘ä»¬çŸ¥é“

$$
  Var \left ( \hat{\beta} _ j | x \right)
  = \frac{\sigma ^2} {SST _1 (1-R _1^2)}
$$

conditional on {\\( [x _{1i}, x _{2i}]: i = 1,\cdots, n \\)}.  
&emsp;æ¡ä»¶ä¸º{\\([x{1i}ï¼Œx{2i}ï¼‰ï¼ši=1ï¼Œ\cdotsï¼Œn\\)}ã€‚

What about the simple regression estimator? Can show  
&emsp;é‚£ä¹ˆç®€å•å›å½’ä¼°è®¡å‘¢ï¼Ÿå¯ä»¥æ˜¾ç¤º

$$
  Var \left ( \widetilde{\beta} _ j | x \right)
  = \frac{\sigma ^2} {SST _1 }
$$

Whenever \\( x _{1i} \\) and \\( x _{2i} \\) are correlated, \\( R _1^2 > 0 \\), and  
&emsp;å½“\\( x _{1i} \\)å’Œ\\( x _{2i} \\)ç›¸å…³æ—¶ï¼Œ\\( R _1^2 > 0 \\)ï¼Œå’Œ

$$
  Var \left ( \widetilde{\beta} _ j | x \right)
  = \frac{\sigma ^2} {SST _1 }
  < \frac{\sigma ^2} {SST _1 (1-R _1^2)}
  < Var( \hat{\beta} _1 | x)
$$

- So, by omitting \\(x _2\\), we can in fact get an estimator with a smaller variance, even though it is biased.  
&emsp;æ‰€ä»¥ï¼Œé€šè¿‡çœç•¥\\(x _2 \\)ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥å¾—åˆ°ä¸€ä¸ªæ–¹å·®è¾ƒå°çš„ä¼°è®¡é‡ï¼Œå³ä½¿å®ƒæ˜¯æœ‰åçš„ã€‚

- When we look at bias and variance, we have a trade-off between simple and multiple regression.  
&emsp;å½“æˆ‘ä»¬è€ƒè™‘åå·®å’Œæ–¹å·®æ—¶ï¼Œæˆ‘ä»¬åœ¨ç®€å•å›å½’å’Œå¤šå…ƒå›å½’ä¹‹é—´è¿›è¡Œäº†æƒè¡¡ã€‚

- Conditioning on the same explanatory variables, we ignored the difference in the variance of the error term.  
&emsp;åœ¨ç›¸åŒçš„è§£é‡Šå˜é‡æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬å¿½ç•¥äº†è¯¯å·®é¡¹æ–¹å·®çš„å·®å¼‚ã€‚

- \\(\sigma ^2\\) will be higher for the simple regression model (conditioning in \\( x _1\\) only)!  
&emsp;å¯¹äºç®€å•å›å½’æ¨¡å‹ï¼Œ\\(\sigma^2\\)å°†æ›´é«˜ï¼ˆä»…åœ¨\\(x _1\\)ä¸­è°ƒèŠ‚ï¼‰ï¼

### Estimating the Error Variance  
&emsp;ä¼°è®¡è¯¯å·®æ–¹å·®

**Theorem: (Unbiased Estimation of \\( \sigma ^2 \\))**  
&emsp;**å®šç†ï¼šï¼ˆæ— åä¼°è®¡\\(\sigma^2\\)**

Under the Gauss-Markov assumptions (MLR.1 through MLR.5)  
&emsp;åœ¨Gauss-Markovå‡è®¾ä¸‹ï¼ˆMLR.1åˆ°MLR.5ï¼‰

$$
  \hat{\sigma} ^2 
  = (n - k - 1) ^{-1} \sum _{i=1}^n \hat{u} _i^2 
  = SSR/df 
$$

is an unbiased estimation of \\( \sigma ^2 \\)  
&emsp;æ˜¯\\(\sigma^2\\)çš„æ— åä¼°è®¡

- The square root of \\( \hat{\sigma} ^2, \hat{\sigma} \\), is reported by all regression packages (**standard error of the regression**, or **RMSE**).  
&emsp;\\(\hat{\sigma}^2ï¼Œ\hat{\sigma}\\)çš„å¹³æ–¹æ ¹ç”±æ‰€æœ‰å›å½’åŒ…æŠ¥å‘Šï¼ˆ**å›å½’çš„æ ‡å‡†è¯¯å·®**ï¼Œæˆ–**RMSE**ï¼‰ã€‚

- Note that *SSR* falls when a new explanatory variable is added, but *df* falls, too. So \\( \hat{\sigma} \\) can increase or decrease!  
&emsp;è¯·æ³¨æ„ï¼Œæ·»åŠ æ–°çš„è§£é‡Šå˜é‡æ—¶ï¼Œ*SSR*ä¼šä¸‹é™ï¼Œä½†*df*ä¹Ÿä¼šä¸‹é™ã€‚æ‰€ä»¥\\(\hat{\sigma}\\)å¯ä»¥å¢åŠ æˆ–å‡å°‘ï¼

### Efficiency of OLS  
&emsp;OLSçš„æ•ˆç‡

**THEOREM 3.4 (Gauss-Markov)**  
&emsp;**å®šç†3.4ï¼ˆé«˜æ–¯-é©¬å°”å¯å¤«ï¼‰**

Under Assumptions MLR.1 through MLR.5, the OLS estimators \\( \hat{\beta} _0, \hat{\beta} _1, \cdots, \hat{\beta} _k \\) are the **best linear unbiased estimators (BLUEs)**.  
&emsp;åœ¨å‡è®¾MLR.1åˆ°MLR.5ä¸‹ï¼ŒOLSä¼°è®¡é‡\\(\hat{\beta}u 0ã€\hat{\beta}u 1ã€\cdotsã€\hat{\beta}u k\\)æ˜¯**æœ€ä½³çº¿æ€§æ— åä¼°è®¡é‡ï¼ˆblueï¼‰**ã€‚

- Best: smaller variance  
&emsp;æœ€ä½³ï¼šæ–¹å·®è¾ƒå°

- Linear: can be expressed as a linear function of { \\(y _1, \cdots, y _n\\)},  
&emsp;çº¿æ€§ï¼šå¯ä»¥è¡¨ç¤ºä¸º{\\(y _1ï¼Œ\cdotsï¼Œy _n\\)}çš„çº¿æ€§å‡½æ•°ï¼Œ

$$
  \hat{\beta} _j = \sum _{i=1}^n w _{ji} y _i
$$

where each \\( w _{ji} \\) is a function of the sample values of all independent variables. 
&emsp;å…¶ä¸­æ¯ä¸€ä¸ª\\( w _{ji} \\)æ˜¯æ‰€æœ‰ç‹¬ç«‹å˜é‡ï¼ˆè‡ªå˜é‡ï¼Ÿï¼‰çš„æ ·æœ¬å€¼çš„å‡½æ•°

### Takeaway  
&emsp;è¯¾å¤–

- Under the Gauss-Markov Assumptions the OLS estimator are unbiased.  
&emsp;åœ¨Gauss-Markovå‡è®¾ä¸‹ï¼ŒOLSä¼°è®¡æ˜¯æ— åçš„ã€‚

- Omitting relevant variables causes OLS to be biased.  
&emsp;å¿½ç•¥ç›¸å…³å˜é‡ä¼šå¯¼è‡´OLSæœ‰åã€‚

- Adding irrelevant variables generally increases the variance of the OLS estimators.  
&emsp;æ·»åŠ ä¸ç›¸å…³çš„å˜é‡é€šå¸¸ä¼šå¢åŠ OLSä¼°è®¡é‡çš„æ–¹å·®ã€‚

## Video 9: Potential Outcomes and Causal Effects  
&emsp;è§†é¢‘9ï¼šæ½œåœ¨ç»“æœå’Œå› æœå…³ç³»

### Causality  
&emsp;å› æœå…³ç³»

- *Causality* has different means to different people.  
&emsp;å› æœå…³ç³»å¯¹ä¸åŒçš„äººæœ‰ä¸åŒçš„æ„ä¹‰ã€‚

- Researchers working in different disciplines have found it useful to think of causal relationships in terms of potential outcomes.  
&emsp;ä»äº‹ä¸åŒå­¦ç§‘çš„ç ”ç©¶äººå‘˜å‘ç°ï¼Œä»æ½œåœ¨ç»“æœçš„è§’åº¦è€ƒè™‘å› æœå…³ç³»æ˜¯å¾ˆæœ‰ç”¨çš„ã€‚

- The difference between these potential outcomes was said to be causal effect of the treatment.  
&emsp;è¿™äº›æ½œåœ¨ç»“æœä¹‹é—´çš„å·®å¼‚è¢«è®¤ä¸ºæ˜¯æ²»ç–—çš„å› æœæ•ˆåº”ã€‚

- We saw that regression can be used to estimate the average treatment effect.  
&emsp;æˆ‘ä»¬çœ‹åˆ°å›å½’å¯ä»¥ç”¨æ¥ä¼°è®¡å¹³å‡æ²»ç–—æ•ˆæœã€‚

- Suppose that there are two individuals, Amy and Ben, and both have the possibility of being high-school graduates or college graduates.  
&emsp;å‡è®¾æœ‰ä¸¤ä¸ªäººï¼Œè‰¾ç±³å’Œæœ¬ï¼Œéƒ½æœ‰å¯èƒ½æ˜¯é«˜ä¸­æ¯•ä¸šç”Ÿæˆ–å¤§å­¦æ¯•ä¸šç”Ÿã€‚

- 2 possible states of the world: college and high-school graduation; 2 potential outcomes specific to each individual.  
&emsp;ä¸–ç•Œä¸Šæœ‰ä¸¤ç§å¯èƒ½çš„çŠ¶æ€ï¼šå¤§å­¦å’Œé«˜ä¸­æ¯•ä¸šï¼›æ¯ä¸ªäººæœ‰ä¸¤ç§å¯èƒ½çš„ç»“æœã€‚

- Amy would have earned $20/hour as college graduated, $10/hour as high school graduate.  
&emsp;è‰¾ç±³å¤§å­¦æ¯•ä¸šæ—¶æ¯å°æ—¶æŒ£20ç¾å…ƒï¼Œé«˜ä¸­æ¯•ä¸šæ—¶æ¯å°æ—¶æŒ£10ç¾å…ƒã€‚

- Ben would have earned $12/hour as college graduated, $8/hour as high school graduate.  
&emsp;æœ¬å¤§å­¦æ¯•ä¸šæ—¶æ¯å°æ—¶æŒ£12ç¾å…ƒï¼Œé«˜ä¸­æ¯•ä¸šæ—¶æ¯å°æ—¶æŒ£8ç¾å…ƒã€‚

- Suppose we have a population of 32 individuals with equal numbers of Amies and Bens.  
&emsp;å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ª32ä¸ªäººçš„ç¾¤ä½“ï¼Œä»–ä»¬æœ‰ç›¸ç­‰æ•°é‡çš„é˜¿ç±³å’Œæœ¬ã€‚

- Suppose further that the individual type is unobservable.  
&emsp;è¿›ä¸€æ­¥å‡è®¾å•ä¸ªç±»å‹æ˜¯ä¸å¯è§‚å¯Ÿçš„ã€‚

- The causal effect of going to college is $10 for Amy, $4 for Ben. The average causal effect is $7.  
&emsp;ä¸Šå¤§å­¦çš„å› æœå…³ç³»æ˜¯è‰¾ç±³10ç¾å…ƒï¼Œæœ¬4ç¾å…ƒã€‚å¹³å‡å› æœæ•ˆåº”ä¸º7ç¾å…ƒã€‚

- Is this what we learn from the regression analysis?  
&emsp;è¿™æ˜¯æˆ‘ä»¬ä»å›å½’åˆ†æä¸­å­¦åˆ°çš„å—ï¼Ÿ

Now suppose that while in high school all students take an aptitude test.  
&emsp;å‡è®¾åœ¨é«˜ä¸­æ—¶æ‰€æœ‰çš„å­¦ç”Ÿéƒ½å‚åŠ äº†èƒ½åŠ›å€¾å‘æµ‹è¯•ã€‚

- If a student gets a high (*H*) score he/she goes to college with probability 3/4.  
&emsp;å¦‚æœä¸€ä¸ªå­¦ç”Ÿè·å¾—é«˜åˆ†ï¼ˆ*H*ï¼‰ï¼Œä»–/å¥¹ä¸Šå¤§å­¦çš„æ¦‚ç‡ä¸º3/4ã€‚

- If a student gets a low (*L*) score he/she goes to college with probability 1/4.  
&emsp;å¦‚æœä¸€ä¸ªå­¦ç”Ÿçš„åˆ†æ•°å¾ˆä½ï¼Œä»–/å¥¹ä¸Šå¤§å­¦çš„æ¦‚ç‡æ˜¯1/4ã€‚

- Suppose further that Amy gets an aptitude score of *H* with probability 3/4. Ben gets a score of *H* with probability 1/4.  
&emsp;è¿›ä¸€æ­¥å‡è®¾è‰¾ç±³çš„èƒ½åŠ›å€¾å‘å¾—åˆ†ä¸º*H*ï¼Œæ¦‚ç‡ä¸º3/4ã€‚æœ¬çš„å¾—åˆ†æ˜¯Hï¼Œæ¦‚ç‡æ˜¯1/4ã€‚

$$
  \begin{align}
    Pr(col | Amy) 
    & = Pr(col | H) Pr(H | Amy) + Pr(col | L) Pr(L | Amy)
    \\\\ & = (3/4) ^2 + (1/4) ^2 = 0.625
  \end{align}
$$

where *col* is the short for college. Similarly, \\( Pr(col \| Ben) = 0.375 \\).  
&emsp;*col*æ˜¯å¤§å­¦çš„ç¼©å†™ã€‚ç±»ä¼¼åœ°ï¼Œ\\(Prï¼ˆcol\| Benï¼‰=0.375\\)ã€‚

![]({{site.url}}/assets/images/2020/ECON5002/distributions.png "Figure 9.1: Distributions")

- Let *col* denote a binary variable taking value 1 for a college graduate, 0 otherwise. From Table 1 we get  
&emsp;è®©*col*è¡¨ç¤ºä¸€ä¸ªäºŒè¿›åˆ¶å˜é‡ï¼Œå¯¹äºå¤§å­¦æ¯•ä¸šç”Ÿå–1ï¼Œå¦åˆ™å–0ã€‚ä»è¡¨1æˆ‘ä»¬å¾—åˆ°

$$
  E[wage | col] = 8.75 + 8.25 col
$$

- $8.25 overstates the average causal effect of $7.  
&emsp;8.25ç¾å…ƒå¤¸å¤§äº†7ç¾å…ƒçš„å¹³å‡å› æœæ•ˆåº”ã€‚

- $8.25 is not the *Average Causal Effect* (ACE) of attending college, but it is the observed difference in realized wages in population.  
&emsp;8.25ç¾å…ƒä¸æ˜¯ä¸Šå¤§å­¦çš„*å¹³å‡å› æœæ•ˆåº”*ï¼ˆACEï¼‰ï¼Œè€Œæ˜¯äººå£ä¸­å®é™…å·¥èµ„çš„è§‚å¯Ÿå·®å¼‚ã€‚

- The decision to attend college is not independent of the unobservable individual type (Amy or Ben).  
&emsp;ä¸Šå¤§å­¦çš„å†³å®šå¹¶ä¸ç‹¬ç«‹äºä¸å¯è§‚å¯Ÿçš„ä¸ªä½“ç±»å‹ï¼ˆè‰¾ç±³æˆ–æœ¬ï¼‰ã€‚

- Additional education is not randomly assigned!  
&emsp;é™„åŠ æ•™è‚²ä¸æ˜¯éšæœºåˆ†é…çš„ï¼

- To estimate the *Average Causal Effect* we need to condition on the appropriate variables.  
&emsp;ä¸ºäº†ä¼°è®¡*å¹³å‡å› æœæ•ˆåº”*æˆ‘ä»¬éœ€è¦åœ¨é€‚å½“çš„å˜é‡ä¸Šè®¾ç½®æ¡ä»¶ã€‚

- The decision to attend the college is based on the aptitude test score, and not on the individual type.  
&emsp;ä¸Šå¤§å­¦çš„å†³å®šæ˜¯åŸºäºèƒ½åŠ›å€¾å‘æµ‹è¯•åˆ†æ•°ï¼Œè€Œä¸æ˜¯ä¸ªäººç±»å‹ã€‚

- Thus education and type are independent once we condition on the test score.  
&emsp;å› æ­¤ï¼Œæ•™è‚²å’Œç±»å‹æ˜¯ç‹¬ç«‹çš„ï¼Œä¸€æ—¦æˆ‘ä»¬å¯¹è€ƒè¯•æˆç»©çš„æ¡ä»¶ã€‚

- Let *hscore* be a binary variable taking value 1 if the student receives a hight test score (*H*), 0 otherwise.  
&emsp;å¦‚æœå­¦ç”Ÿè·å¾—é«˜åˆ†ï¼ˆ*H*ï¼‰ï¼Œåˆ™å°†*hscore*è®¾ä¸ºäºŒè¿›åˆ¶å˜é‡ï¼Œå–å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚

- Denote \\( wage _i(1), wage _i(0) \\) the potential outputs for \\( col _i = 1 \text{ and } col _i = 0 \\), respectively.  
&emsp;åˆ†åˆ«è¡¨ç¤º\ \\(wage _iï¼ˆ1ï¼‰ï¼Œwage _iï¼ˆ0ï¼‰\\)ä¸º\\(col _i=1 \text{å’Œ} col _i=0\\)çš„æ½œåœ¨è¾“å‡ºã€‚

- **Conditional Independence Assumption (CIA)**  
&emsp;**æ¡ä»¶ç‹¬ç«‹å‡è®¾ï¼ˆCIAï¼‰**

$$
  [wage _i(1), wage _i(0)] \models col _i | hscore _i
$$

- Among the students with a hight test score, 3/4 are Amies and 1/4 are Bens.  
&emsp;åœ¨è€ƒè¯•æˆç»©é«˜çš„å­¦ç”Ÿä¸­ï¼Œæœ‰3/4çš„å­¦ç”Ÿæˆç»©å·®ï¼Œ1/4çš„å­¦ç”Ÿæˆç»©å·®ã€‚

- Thus, the ACE for students with score *H* is  
&emsp;å› æ­¤ï¼Œå¾—åˆ†ä¸º*H*çš„å­¦ç”Ÿçš„å¾—åˆ†ä¸º

$$
  (3/4) \times (20 - 10) + (1/4) \times (12 - 8) = $8.5.
$$

- Among the students with a low test score, 1/4 are Amies and 3/4 are Bens.  
&emsp;åœ¨è€ƒè¯•æˆç»©ä½çš„å­¦ç”Ÿä¸­ï¼Œ1/4çš„å­¦ç”Ÿæ˜¯â€œé˜¿ç¾â€ï¼Œ3/4çš„å­¦ç”Ÿæ˜¯â€œæœ¬â€ã€‚

- Thus, the ACE for students with score L is  
&emsp;å› æ­¤ï¼Œå¾—åˆ†ä¸ºLçš„å­¦ç”Ÿçš„å¾—åˆ†æ˜¯

$$
  (3/4) \times (20 - 10) + (3/4) \times (12 - 8) = $5.5.
$$

  - Would we be able to learn the ACE from a regression analysis?  
  &emsp;æˆ‘ä»¬èƒ½ä»å›å½’åˆ†æä¸­å­¦ä¹ ACEå—ï¼Ÿ

![]({{site.url}}/assets/images/2020/ECON5002/jointDistribution.png "Figure 9.2: Joint Distributions")

From the table above we compute  
&emsp;ä»ä¸Šè¡¨æˆ‘ä»¬å¯ä»¥è®¡ç®—

$$
  E[wage | col, hscore]
  = 8.50 + 1.00 hscore + 5.50 col + 3.00 (hscore \times col)
$$

The ACE for students with low test scores is indeed  
&emsp;è€ƒè¯•åˆ†æ•°ä½çš„å­¦ç”Ÿçš„å¹³å‡å› æœæ•ˆåº”ç¡®å®å¦‚æ­¤

$$
  E[wage(1) | hscore = 0] - E[wage(0) | hscore = 0] = $5.50
$$

Next we consider  
&emsp;æ¥ä¸‹æ¥æˆ‘ä»¬è€ƒè™‘

$$
  E[wage(1) | hscore = 1] - E[wage(0) | hscore = 1] = $8.50
$$

Since half of the students achieve a high score test, and half a low score test  
&emsp;å› ä¸ºä¸€åŠçš„å­¦ç”Ÿè¾¾åˆ°äº†é«˜åˆ†æµ‹è¯•ï¼Œè€Œä¸€åŠçš„å­¦ç”Ÿè¾¾åˆ°äº†ä½åˆ†æµ‹è¯•

$$
  (5.50 + 8.50) \times 1/2 = $7,
$$

that is average causal effect in the population.  
&emsp;è¿™æ˜¯äººå£ä¸­çš„å¹³å‡å› æœæ•ˆåº”ã€‚

Similarly, we could consider the regression model  
&emsp;åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘å›å½’æ¨¡å‹

$$
  wage(col, hscore, u) = 8.50 + 1.00 hscore + 5.50 col + 3.00 ( hscore \times col) + u
$$

If *u* and *hscore* remain fixed when *col* changes, then the causal effect of *col* on *wage* can be computed as  
&emsp;å¦‚æœ*col*æ”¹å˜æ—¶*u*å’Œ*hscore*ä¿æŒä¸å˜ï¼Œ*col*å¯¹*wage*çš„å› æœå…³ç³»å¯ä»¥è®¡ç®—ä¸º

$$
  wage(1, hscore, u) - wage(0, hscore, u) 
  = \begin{cases}
    $8.50 & \text{if  } hscore = 1,
    \\\\ $5.50 & \text{if } hscore = 0.
  \end{cases}
$$

Let  
&emsp;è®©

$$
  y = \beta _0 + \beta _1 x _1 + \beta _2 x _2
    = \ell ( x _1, x _2, u)
$$

The causal effect of x1 on y is sometimes defined as  
&emsp;x1å¯¹yçš„å› æœå…³ç³»æœ‰æ—¶è¢«å®šä¹‰ä¸º

$$
  \nabla \ell ( x _1, x _2, u),
$$

the change in *y* due a change in \\(x _1\\), holding \\(x _2\\) and *u* constant, where  
&emsp;ç”±äº \\(x _1\\)ã€ä¿æŒ\\(x _2\\)å’Œ*u*å¸¸æ•°çš„å˜åŒ–è€Œå¼•èµ·çš„*y*çš„å˜åŒ–ï¼Œå…¶ä¸­

$$
  \nabla \ell ( x _1, x _2, u) =
  \begin{cases}
    \ell ( 1, x _2, u) - \ell ( 0, x _2, u) & \text{x binary}
    \\\\ \ell ( x _1 +1, x _2, u) - \ell ( x _1, x _2, u) & \text{x discrete}
    \\\\ \frac{\partial{\ell ( x _1, x _2, u) }}{\partial{x _1}} & \text{x continous}
  \end{cases}
$$

### Takeaway  
&emsp;è¯¾å¤–

- If the CIA holds, we can use the regression to estimate causal effects.  
&emsp;å¦‚æœæ¡ä»¶ç‹¬ç«‹çš„å‡è®¾æˆç«‹ï¼Œå¯ä»¥ä½¿ç”¨å›å½’æ¥ä¼°è®¡å› æœå…³ç³»ã€‚

- Conditioning on the aptitude test score, education can be treated as randomly assigned.  
&emsp;æ ¹æ®èƒ½åŠ›å€¾å‘æµ‹è¯•åˆ†æ•°ï¼Œæ•™è‚²å¯ä»¥è¢«è§†ä¸ºéšæœºåˆ†é…ã€‚

## Video 10: Multiple Regression Analysis: Further Issues  
&emsp;è§†é¢‘10ï¼šå¤šå…ƒå›å½’åˆ†æï¼šè¿›ä¸€æ­¥çš„é—®é¢˜

### Models with Quadratics  
&emsp;äºŒæ¬¡æ¨¡å‹

- Assume we have estimated the model  
&emsp;å‡è®¾æˆ‘ä»¬å·²ç»ä¼°è®¡äº†æ¨¡å‹

$$
  \begin{array}{m}
    \widehat{lwage} = \underbrace{-.192} _{(.291)} + \underbrace{.136} _{(.011)} educ + \underbrace{.123} _{(.037)} exper - \underbrace{.0038} _{(.0017)} exper ^2
    \\\\n = 759 \text{, } R ^2 = .196
  \end{array}
$$

- The estimated return to education is 13.6%. The model assumes this is the same for all years of experience and education.  
&emsp;æ•™è‚²å›æŠ¥ç‡ä¼°è®¡ä¸º13.6%ã€‚è¯¥æ¨¡å‹å‡è®¾è¿™å¯¹äºæ‰€æœ‰å¹´çš„ç»éªŒå’Œæ•™è‚²éƒ½æ˜¯ç›¸åŒçš„ã€‚

- On average, an additional year of education increases the wage by 13.6%, regardless present level of education.  
&emsp;ä¸ç®¡ç›®å‰çš„æ•™è‚²æ°´å¹³å¦‚ä½•ï¼Œå¹³å‡æ¥è¯´ï¼Œå¤šå—ä¸€å¹´æ•™è‚²ä¼šä½¿å·¥èµ„å¢åŠ 13.6%ã€‚

- By contrast, each year of experience is worth less than the preceding year.  
&emsp;ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¯ä¸€å¹´çš„ç»éªŒéƒ½ä¸å¦‚å‰ä¸€å¹´ã€‚

- Taking the partial derivative with respect to exper, we find  
&emsp;å¯¹äºexperçš„åå¯¼æ•°ï¼Œæˆ‘ä»¬å‘ç°

$$
  \frac{\Delta \widehat{lwage}}{\Delta exper}
  \approx .123 - 2(.0038) exper 
  = .123 - .0076 exper 
  \tag{10.1}
$$

- We can think of 12:3% as approximately the return to the first year of experience { essentially starting off in the workforce.  
&emsp;æˆ‘ä»¬å¯ä»¥æŠŠ12:3%çœ‹ä½œæ˜¯å¯¹ç¬¬ä¸€å¹´å·¥ä½œç»éªŒçš„å¤§è‡´å›æŠ¥{åŸºæœ¬ä¸Šæ˜¯ä»åŠ³åŠ¨åŠ›å¼€å§‹çš„ã€‚

- The return in going from 10 to 11 is about  
&emsp;ä»10ç‚¹åˆ°11ç‚¹çš„å›æŠ¥å¤§çº¦æ˜¯

$$
  .123 - .0076(10) = .0.47
$$

or 4.7%  
&emsp;æˆ–è€…4.7%

- We could be more precise. Holding educ fixed  
&emsp;æˆ‘ä»¬å¯ä»¥æ›´åŠ ç²¾ç¡®ä¸€äº›ï¼Œä¿è¯educä¸å˜

$$
  \begin{array}{l}
    \quad lwage(exp =1) -lwage(exp =10) \tag{10.2}
    \\\\ = [.123(11) - .0038(11) ^2] - [.123(10) - .0038(10)^2] = .043
  \end{array}
$$

or 4.3%, which is reasonably close.  
&emsp;æˆ–è€…4.3%ï¼Œ ç›¸å½“æ¥è¿‘ã€‚

- The quadratic function  
&emsp;äºŒæ¬¡å‡½æ•°

$$
  .123 exper - 0.0038 exper ^2 \tag{10.3}
$$

turns at about  
&emsp;å¤§çº¦è½¬åŒ–ä¸º

$$
  exper ^* = .123/[2 \cdot (.0038)] \approx 16.2 \tag{10.4}
$$

- But fewer than 2% of the observations have exper > 16, so not much worry.  
&emsp;ä½†åªæœ‰ä¸åˆ°2%çš„è§‚å¯Ÿç»“æœæ˜¾ç¤ºexper>16ï¼Œæ‰€ä»¥ä¸ç”¨å¤ªæ‹…å¿ƒã€‚

![]({{site.url}}/assets/images/2020/ECON5002/quadraticRelationship.png "Figure 10.1: Quadratic relationship between lwage and experience.")

### Models with Interaction Terms  
&emsp;å…·æœ‰äº¤äº’é¡¹çš„æ¨¡å‹

- Suppose we have two explanatory variables and start with the usual model:  
&emsp;å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªè§£é‡Šå˜é‡ï¼Œä»é€šå¸¸çš„æ¨¡å‹å¼€å§‹ï¼š

$$
  y = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + u
$$

- The partial effect (PE) of \\(x _1\\) on y is \\(\beta _1\\) and the PE of \\(x _2\\) on y is \\(\beta _2\\).  
&emsp;\\(x _1\\)å¯¹yçš„éƒ¨åˆ†æ•ˆåº”ï¼ˆPEï¼‰ä¸º\\(\beta _1\\)ï¼Œè€Œ\\(x _2\\)å¯¹yçš„éƒ¨åˆ†æ•ˆåº”ï¼ˆPEï¼‰ä¸º\\(\beta _2\\)ã€‚

- Sometimes it is natural to think the partial effect of one variable, say education, could depend on the level of another variable, say intelligence.  
&emsp;æœ‰æ—¶ï¼Œäººä»¬å¾ˆè‡ªç„¶åœ°è®¤ä¸ºï¼Œä¸€ä¸ªå˜é‡ï¼ˆæ¯”å¦‚æ•™è‚²ï¼‰çš„éƒ¨åˆ†å½±å“å¯èƒ½å–å†³äºå¦ä¸€ä¸ªå˜é‡ï¼ˆæ¯”å¦‚æ™ºåŠ›ï¼‰çš„æ°´å¹³ã€‚

- We add an **interaction term**, \\(x _1 x _2\\), to the usual model:  
&emsp;æˆ‘ä»¬åœ¨é€šå¸¸çš„æ¨¡å‹ä¸­æ·»åŠ äº†ä¸€ä¸ª**äº¤äº’é¡¹**ï¼Œ\\(x _1 x _2\\)ï¼š

$$
  y = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \beta _3 x _1 x _2 + u
$$

- Holding \\(x _2\\)  (and *u*) fixed, the partial effect of \\(x _1\\)  on *y* is now  
&emsp;ä¿æŒ\\(x _2\\) ï¼ˆå’Œ*u*ï¼‰å›ºå®šï¼Œ\\(x _1\\)å¯¹*y*çš„éƒ¨åˆ†å½±å“ç°åœ¨æ˜¯

$$
  \frac{\Delta y}{\Delta x _1}
  \approx \beta _1 + \beta _3 x _2
$$

so that effect of \\(x _1\\) depends on \\(x _2\\) unless \\(\beta _3 = 0\\)  
&emsp;æ‰€ä»¥\\(x _1\\)çš„æ•ˆåº”å–å†³äº\\(x _2\\)ï¼Œé™¤é\\(\beta _3=0\\)

- Similarly,  
&emsp;åŒæ ·åœ°ï¼Œ

$$
  \frac{\Delta y}{\Delta x _2}
  = \beta _2 + \beta _3 x _1
$$

**Example**  
&emsp;**ç¤ºä¾‹**

- Do education and IQ have an interactive effect in the ln(*wage*) equation?  
&emsp;æ•™è‚²å’Œæ™ºå•†åœ¨lnï¼ˆ*wage*ï¼‰ç­‰å¼ä¸­æœ‰äº¤äº’ä½œç”¨å—ï¼Ÿ

$$
  \widehat{lwage} = -.762 + .195 educ + .022 IQ - . 001(educ \cdot IQ) \tag{10.6}
$$

- According to these estimates, schooling is worth more for those with lower intelligence.  
&emsp;æ ¹æ®è¿™äº›ä¼°è®¡ï¼Œå¯¹é‚£äº›æ™ºåŠ›è¾ƒä½çš„äººæ¥è¯´ï¼Œä¸Šå­¦æ›´æœ‰ä»·å€¼ã€‚

$$
  \frac{\Delta \widehat{lwage}}{\Delta educ}
  \approx .195 - .001 IQ
$$

- The interpretation of the parameters on the original variables can be tricky.  
&emsp;åŸå§‹å˜é‡çš„å‚æ•°è§£é‡Šå¯èƒ½å¾ˆæ£˜æ‰‹ã€‚

- \\(\beta _1\\) would be interpreted as the effect of an additional year of education for those with *IQ* = 0.  
&emsp;\\(\beta _ 1\\)å°†è¢«è§£é‡Šä¸ºå¯¹é‚£äº›æ™ºå•†ä¸º0çš„äººé¢å¤–æ¥å—ä¸€å¹´æ•™è‚²çš„å½±å“ã€‚

- This effect is not of much practical interest!  
&emsp;è¿™ç§æ•ˆæœæ²¡æœ‰å¤šå¤§å®é™…æ„ä¹‰ï¼

### Goodness-of-Fit  
&emsp;æ‹Ÿåˆä¼˜åº¦

- Using the same set of data and the same dependent variable, the \\(R ^2\\) can never fall when another independent variable is added to the regression.  
&emsp;ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†å’Œç›¸åŒçš„å› å˜é‡ï¼Œå½“å¦ä¸€ä¸ªè‡ªå˜é‡åŠ å…¥å›å½’æ—¶ï¼Œ\\(R^2\\)æ°¸è¿œä¸ä¼šä¸‹é™ã€‚

- This means that, if we focus on \\(R ^2\\), we might include silly variables among the \\(x _j\\) .  
&emsp;è¿™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬å…³æ³¨\\(R^2 \\)ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šåœ¨\\(x _j \\)ä¸­åŒ…å«æ„šè ¢çš„å˜é‡ã€‚

- Adding another *x* cannot make *SSR* increase. The *SSR* falls unless the coecient on the new variable is identically zero.  
&emsp;å†åŠ ä¸€ä¸ª*x*ä¸èƒ½ä½¿*SSR*å¢åŠ ã€‚é™¤éæ–°å˜é‡çš„ç³»æ•°ç›¸åŒä¸ºé›¶ï¼Œ*SSR*ä¸‹é™ã€‚

- Adding regressors, decreases the number of degrees of freedom.  
&emsp;æ·»åŠ å›å½’ï¼Œå‡å°‘è‡ªç”±åº¦çš„æ•°é‡ã€‚

### Adjusted R-Squared  
&emsp;è°ƒæ•´Rå¹³æ–¹

- Sometimes we want to compare models for the same dependent variable.  
&emsp;æœ‰æ—¶æˆ‘ä»¬è¦æ¯”è¾ƒåŒä¸€å› å˜é‡çš„æ¨¡å‹ã€‚

- Comparing non-nested models using the \\(R ^2\\) can be unfair if the number of regressors is different.  
&emsp;å¦‚æœå›å½’æ•°ä¸åŒï¼Œä½¿ç”¨\\(R^2\\)æ¯”è¾ƒéåµŒå¥—æ¨¡å‹å¯èƒ½ä¸å…¬å¹³ã€‚

- The adjusted \\(R ^2\\) can be helpful in these cases.  
&emsp;åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œè°ƒæ•´åçš„\\(R^2\\)å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚

- As usual, start with  
&emsp;åƒå¾€å¸¸ä¸€æ ·ï¼Œä»

$$
  y = \beta _0 + \beta _1 x _1 + \cdots +\beta _k x _k + u \tag{10.7}
$$

- The formula for the \\(R ^2\\) can be written as  
&emsp;\\(R ^2\\)çš„å…¬å¼å¯ä»¥è¢«å†™ä½œ

$$
  R ^2 = 1- \frac{SSR}{SST} = 1 - \frac{SSR/n}{SST/n} \tag{10.8}
$$

- We think of \\(R ^2\\) as using *SSR/n* to estimate *Var(u)* and *SST/n* to estimate *Var(y)*.  
&emsp;æˆ‘ä»¬è®¤ä¸º\\(R^2\\)æ˜¯ä½¿ç”¨*SSR/n*æ¥ä¼°è®¡*Varï¼ˆuï¼‰*ï¼Œ*SST/n*æ¥ä¼°è®¡*Varï¼ˆyï¼‰*ã€‚

- These estimators are consistent, i.e.  
&emsp;è¿™äº›ä¼°è®¡æ˜¯ä¸€è‡´çš„ï¼Œå³ã€‚

$$
  \frac{SSR}{n} \xrightarrow p Var(u), 
  \ \frac{SST}{n} \xrightarrow p Var(y),
$$

but biased.  
&emsp;ä½†æœ‰åè§ã€‚

- Instead, use  
&emsp;ç›¸åï¼Œä½¿ç”¨

$$
  \frac{SSR}{n-k-1} \text{  and  } \frac{SST}{n-1}
$$

as the unbiased estimators.  
&emsp;ä½œä¸ºæ— åä¼°è®¡é‡ã€‚

The **adjusted R-squared** is defined as  
&emsp;**è°ƒæ•´åçš„Rå¹³æ–¹**å®šä¹‰ä¸º

$$
  \begin{align}
    \overline{R} ^2 
    & = 1 - \frac{[SSR / (n-k-1)]}{[SST / (n-1)]} \tag{10.9} 
    \\\\ & = 1 - \frac{\hat{\sigma} ^2}{\hat{\sigma} _y^2} \tag{10.10}
  \end{align}
$$

where \\( \hat{\sigma} ^2\\) is the usual variance parameter estimator of \\( Var(u _i) \\).  
&emsp;å…¶ä¸­\\(\hat{\sigma}^2\\)æ˜¯\\(Varï¼ˆu _iï¼‰\\)çš„å¸¸ç”¨æ–¹å·®å‚æ•°ä¼°è®¡é‡ã€‚

- When more regressors are added, *SSR* falls, but so does \\( df = n - k -1. \ \overline{R} ^2\\) can increase or decrease.  
&emsp;å½“åŠ å…¥æ›´å¤šçš„å›å½’å˜é‡æ—¶ï¼Œ*SSR*ä¸‹é™ï¼Œä½†\\(df=n-k-1ï¼‰ä¹Ÿä¸‹é™ã€‚\\overline{R}^2\\)å¯ä»¥å¢åŠ æˆ–å‡å°‘ã€‚


- For \\( k > 1, \overline{R} ^2 < R ^2 \\) (unless *SSR* = 0).  
&emsp;å¯¹äº\\(k>1ï¼Œ{R}^2<R^2\\)ï¼ˆé™¤é*SSR*=0ï¼‰ã€‚


- It is possible that \\( \overline{R} ^2 < 0 \\), especially if \\(df\\) is small.  
&emsp;æœ‰å¯èƒ½æ˜¯\\(\overline{R}^2<0\\)ï¼Œç‰¹åˆ«æ˜¯å¦‚æœ\\(df\\)å¾ˆå°ã€‚


- \\( R ^2 > 0 \\) always.  
&emsp;ä¸Šå¼å§‹ç»ˆæˆç«‹ã€‚

### Takeaway  
&emsp;è¯¾å¤–

- Quadratic and interaction terms make the regression function more fexible but the interpretation of the parameters can be tricky.  
&emsp;äºŒæ¬¡é¡¹å’Œäº¤äº’é¡¹ä½¿å›å½’å‡½æ•°æ›´çµæ´»ï¼Œä½†å‚æ•°çš„è§£é‡Šå¯èƒ½å¾ˆæ£˜æ‰‹ã€‚

- The adjusted R-squared penalizes for adding regressors with little explanatory power.  
&emsp;è°ƒæ•´åçš„R-å¹³æ–¹æƒ©ç½šå¢åŠ å›å½’å‡ ä¹æ²¡æœ‰è§£é‡ŠåŠ›ã€‚

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

